# Model Comparison Script

The `onnx_vs_sentence_transformer_comparison.py` script compares embeddings generated by ONNX, OpenVINO, and HuggingFace (SentenceTransformer) implementations.

## Features

- **Compare multiple model formats**: ONNX, OpenVINO, and HuggingFace
- **Performance benchmarking**: Measure inference speed across formats
- **Embedding similarity analysis**: Compute cosine similarity, MSE, and Manhattan distance
- **Flexible pooling**: Supports both CLS token and mean pooling
- **Batch processing**: Efficient processing of large datasets

## Usage

### Basic Examples

#### Compare OpenVINO with HuggingFace

```bash
python scripts/onnx_vs_sentence_transformer_comparison.py \
  --openvino granite-embedding-openvino-int8/granite_embedding_fp32.xml \
  --transformer ibm-granite/granite-embedding-english-r2 \
  --input /path/to/sentences.txt \
  --batch_size 32
```

#### Compare ONNX with HuggingFace

```bash
python scripts/onnx_vs_sentence_transformer_comparison.py \
  --onnx /path/to/onnx_model/ \
  --transformer ibm-granite/granite-embedding-english-r2 \
  --input /path/to/sentences.txt \
  --batch_size 32
```

#### Compare All Three Formats

```bash
python scripts/onnx_vs_sentence_transformer_comparison.py \
  --onnx /path/to/onnx_model/ \
  --openvino granite-embedding-openvino-int8/granite_embedding_fp32.xml \
  --transformer ibm-granite/granite-embedding-english-r2 \
  --input /path/to/sentences.txt \
  --batch_size 32
```

#### Control CPU Threads for Fair Benchmarking

```bash
# Compare with controlled thread count (e.g., 4 threads)
python scripts/onnx_vs_sentence_transformer_comparison.py \
  --openvino granite-embedding-openvino-int8/granite_embedding_fp32.xml \
  --transformer ibm-granite/granite-embedding-english-r2 \
  --input /path/to/sentences.txt \
  --device cpu \
  --num_threads 4 \
  --openvino_threads 4 \
  --batch_size 32
```

#### Test Different Precision Levels

```bash
# Test with INT4 quantization (fastest)
python scripts/onnx_vs_sentence_transformer_comparison.py \
  --openvino granite-embedding-openvino-int8/granite_embedding_fp32.xml \
  --transformer ibm-granite/granite-embedding-english-r2 \
  --input /path/to/sentences.txt \
  --openvino_precision int4 \
  --device cpu

# Test with INT8 quantization (good balance)
python scripts/onnx_vs_sentence_transformer_comparison.py \
  --openvino granite-embedding-openvino-int8/granite_embedding_fp32.xml \
  --transformer ibm-granite/granite-embedding-english-r2 \
  --input /path/to/sentences.txt \
  --openvino_precision int8 \
  --device cpu

# Test with FP16 (GPU optimized)
python scripts/onnx_vs_sentence_transformer_comparison.py \
  --openvino granite-embedding-openvino-int8/granite_embedding_fp32.xml \
  --transformer ibm-granite/granite-embedding-english-r2 \
  --input /path/to/sentences.txt \
  --openvino_precision fp16 \
  --openvino_device GPU
```

## Command-Line Options

### Required Arguments

- `--transformer MODEL`: HuggingFace model name or path (used as baseline)
- `--input FILE`: Path to input file with sentences (text or JSONL format)
- At least one of: `--onnx` or `--openvino`

### Model Options

- `--onnx PATH`: Path to ONNX model directory
- `--openvino PATH`: Path to OpenVINO model file (.xml)
- `--openvino_tokenizer PATH`: Path to tokenizer directory (default: model directory)
- `--openvino_precision {int4,int8,fp16,fp32}`: OpenVINO precision hint for inference
- `--int8`: Use INT8 precision hint (deprecated, use `--openvino_precision int8`)
- `--openvino_device DEVICE`: OpenVINO device (CPU, GPU, etc.) (default: CPU)

**OpenVINO Precision Options**:
- `int4`: 4-bit integer quantization (fastest, smallest, lower accuracy)
- `int8`: 8-bit integer quantization (fast, small, good accuracy)
- `fp16`: 16-bit floating point (balanced speed/accuracy)
- `fp32`: 32-bit floating point (default, best accuracy)

### Processing Options

- `--num_samples N`: Number of samples to use (default: all)
- `--batch_size N`: Batch size for processing (default: 128)
- `--max_text_length N`: Maximum text length in characters
- `--pooling {cls,mean}`: Pooling method for OpenVINO (default: cls)
  - `cls`: Use CLS token (first token) - **recommended for Granite models**
  - `mean`: Use mean pooling over all tokens

### Device Options

- `--device {cuda,cpu}`: Device for ONNX and HuggingFace models (default: cuda)

### Performance Tuning Options

- `--num_threads N`: Number of CPU threads for PyTorch (default: all available)
- `--openvino_threads N`: Number of CPU threads for OpenVINO (default: all available)

**Note**: Thread control only applies when running on CPU. These options are useful for:
- Fair benchmarking across different model formats
- Resource-constrained environments
- Preventing over-subscription in multi-process scenarios

## Input File Formats

The script accepts two input formats:

### 1. Plain Text (one sentence per line)

```text
Machine learning is a subset of artificial intelligence.
Natural language processing enables computers to understand human language.
Deep learning models use neural networks with multiple layers.
```

### 2. JSONL Format

```jsonl
{"text": "Machine learning is a subset of artificial intelligence."}
{"text": "Natural language processing enables computers to understand human language."}
{"text": "Deep learning models use neural networks with multiple layers."}
```

## Output

The script outputs two main sections:

### 1. Performance Comparison

```
======================================================================
PERFORMANCE COMPARISON
======================================================================
HuggingFace SentenceTransformer: 1.50s
ONNX Runtime:                    0.45s (speedup: 3.33x)
OpenVINO:                        0.35s (speedup: 4.29x)

OpenVINO vs ONNX:                1.29x faster
```

### 2. Embedding Similarity Comparison

```
======================================================================
EMBEDDING SIMILARITY COMPARISON
======================================================================

ONNX vs HuggingFace:
  Mean Cosine Similarity:  0.999845 (1.0 = identical)
  Mean Squared Error:      0.000012 (0.0 = identical)
  Mean Manhattan Distance: 0.000234 (0.0 = identical)
  → ONNX produces very similar embeddings! ✓

OpenVINO vs HuggingFace:
  Mean Cosine Similarity:  1.000000 (1.0 = identical)
  Mean Squared Error:      0.000000 (0.0 = identical)
  Mean Manhattan Distance: 0.000000 (0.0 = identical)
  → OpenVINO produces very similar embeddings! ✓

ONNX vs OpenVINO:
  Mean Cosine Similarity:  0.999845 (1.0 = identical)
  Mean Squared Error:      0.000012 (0.0 = identical)
  Mean Manhattan Distance: 0.000234 (0.0 = identical)
  → ONNX/OpenVINO produces very similar embeddings! ✓
```

## Understanding the Metrics

### Cosine Similarity
- Range: -1 to 1 (1 = identical direction)
- > 0.99: Very similar embeddings ✓
- 0.95-0.99: Similar with minor differences
- < 0.95: Significant differences

### Mean Squared Error (MSE)
- Range: 0 to ∞ (0 = identical)
- Measures average squared difference
- Lower is better

### Manhattan Distance
- Range: 0 to ∞ (0 = identical)
- Sum of absolute differences
- Lower is better

## Important Notes

### Pooling Method

**CRITICAL**: Different models use different pooling strategies. Always check the model's configuration:

- **IBM Granite models** use **CLS token pooling** (--pooling cls)
- Many other models use **mean pooling** (--pooling mean)

To check a model's pooling configuration:

```python
from huggingface_hub import hf_hub_download
import json

config_path = hf_hub_download(
    repo_id='ibm-granite/granite-embedding-english-r2',
    filename='1_Pooling/config.json'
)
with open(config_path) as f:
    print(json.load(f))
```

### Performance Considerations

- **Small datasets** (< 1000 samples): May show overhead, OpenVINO might be slower
- **Large datasets** (> 10000 samples): OpenVINO and ONNX show significant speedups
- **Fixed batch size models**: OpenVINO models with batch_size=1 process items sequentially
- **GPU vs CPU**: Use `--device cuda` for ONNX/HuggingFace on GPU, OpenVINO typically runs on CPU

### Troubleshooting

#### Low Cosine Similarity (< 0.95)

1. **Check pooling method**: Try both `--pooling cls` and `--pooling mean`
2. **Verify model versions**: Ensure ONNX/OpenVINO were converted from same source
3. **Check tokenization**: Verify tokenizer is compatible
4. **Inspect first few embeddings**: Add debug prints to check values

#### OpenVINO Slower than Expected

1. **Small dataset**: Normal for < 1000 samples due to compilation overhead
2. **Fixed batch size**: Model has batch_size=1, processes items one-at-a-time
3. **Device selection**: Try `--openvino_device GPU` if available
4. **Enable INT8**: Use `--int8` flag for INT8 models

#### Out of Memory

1. Reduce `--batch_size`
2. Limit dataset with `--num_samples`
3. Use `--max_text_length` to truncate long texts

#### Inconsistent Performance / Thread Contention

If you see inconsistent performance or CPU usage issues:

1. **Set explicit thread counts** for reproducible benchmarks:
   ```bash
   --num_threads 4 --openvino_threads 4
   ```

2. **Match thread counts across all models** for fair comparison

3. **Avoid over-subscription**: If you have 8 cores, don't use more than 8 threads total

4. **Check actual thread usage**:
   ```bash
   # Monitor during execution
   htop  # or top, to see actual CPU usage
   ```

## Example Workflow

### 1. Convert Model to OpenVINO

```bash
python scripts/granite_to_openvino_int8.py \
  --model_name ibm-granite/granite-embedding-english-r2 \
  --output_dir granite-openvino \
  --calibration_samples 300
```

### 2. Prepare Test Data

```bash
# Create a test file with sample sentences
cat > test_data.txt << EOF
Machine learning is transforming industries.
Natural language processing enables human-computer interaction.
Deep learning models achieve state-of-the-art results.
EOF
```

### 3. Run Comparison

```bash
python scripts/onnx_vs_sentence_transformer_comparison.py \
  --openvino granite-openvino/granite_embedding_fp32.xml \
  --transformer ibm-granite/granite-embedding-english-r2 \
  --input test_data.txt \
  --pooling cls \
  --batch_size 16 \
  --device cpu
```

### 4. Analyze Results

Check the output for:
- ✓ Cosine similarity > 0.99
- ✓ Speedup > 1.0x for larger datasets
- ✓ Consistent embedding values

## Integration Example

```python
from openvino.runtime import Core
from transformers import AutoTokenizer
import numpy as np

# Load OpenVINO model
core = Core()
model = core.read_model('granite-openvino/granite_embedding_fp32.xml')
compiled_model = core.compile_model(model, 'CPU')
tokenizer = AutoTokenizer.from_pretrained('granite-openvino')

def get_embeddings(texts):
    """Get embeddings using OpenVINO with CLS pooling."""
    embeddings = []

    for text in texts:
        # Tokenize
        inputs = tokenizer(
            [text],
            padding='max_length',
            truncation=True,
            return_tensors='np',
            max_length=512
        )

        # Prepare inputs
        input_data = {
            'input_ids': inputs['input_ids'].astype(np.int64),
            'attention_mask': inputs['attention_mask'].astype(np.int64)
        }

        # Run inference
        result = compiled_model(input_data)
        output = result[list(result.keys())[0]]

        # CLS pooling (first token)
        cls_embedding = output[0, 0, :]

        # Normalize
        cls_embedding = cls_embedding / np.linalg.norm(cls_embedding)

        embeddings.append(cls_embedding)

    return np.array(embeddings)

# Use it
texts = ["Machine learning is amazing", "Deep learning is powerful"]
embeddings = get_embeddings(texts)
print(f"Embeddings shape: {embeddings.shape}")
```

## References

- [OpenVINO Documentation](https://docs.openvino.ai/)
- [ONNX Runtime Documentation](https://onnxruntime.ai/docs/)
- [SentenceTransformers Documentation](https://www.sbert.net/)
- [IBM Granite Models](https://huggingface.co/ibm-granite)
