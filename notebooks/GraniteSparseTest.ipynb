{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Granite Sparse Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingest and Retriece using PyMilvus\n",
    "\n",
    "`pip install pymilvus[model]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': '456894915525738502', 'distance': 12.364130020141602, 'entity': {'id': 'item_0'}}]\n",
      "[{'id': '456894915525738504', 'distance': 17.1358699798584, 'entity': {'id': 'item_2'}}]\n"
     ]
    }
   ],
   "source": [
    "from pymilvus import model\n",
    "from pymilvus import MilvusClient, DataType\n",
    "\n",
    "client = MilvusClient(\"./milvus_demo.db\")\n",
    "\n",
    "client.drop_collection(collection_name=\"my_sparse_collection\")\n",
    "\n",
    "schema = client.create_schema(\n",
    "    auto_id=True,\n",
    "    enable_dynamic_fields=True,\n",
    ")\n",
    "\n",
    "schema.add_field(field_name=\"pk\", datatype=DataType.VARCHAR, is_primary=True, max_length=100)\n",
    "schema.add_field(field_name=\"id\", datatype=DataType.VARCHAR, is_primary=False, max_length=100)\n",
    "schema.add_field(field_name=\"embeddings\", datatype=DataType.SPARSE_FLOAT_VECTOR)\n",
    "\n",
    "index_params = client.prepare_index_params()\n",
    "\n",
    "index_params.add_index(field_name=\"embeddings\",\n",
    "                               index_name=\"sparse_inverted_index\",\n",
    "                               index_type=\"SPARSE_INVERTED_INDEX\",\n",
    "                               metric_type=\"IP\",\n",
    "                               params={\"drop_ratio_build\": 0.2})\n",
    "client.create_collection(\n",
    "    collection_name=\"my_sparse_collection\",\n",
    "    schema=schema,\n",
    "    index_params=index_params\n",
    ")\n",
    "\n",
    "embeddings_model = model.sparse.SpladeEmbeddingFunction(\n",
    "    model_name=\"ibm-granite/granite-embedding-30m-sparse\", \n",
    "    device=\"cpu\",\n",
    "    batch_size=2,\n",
    "    k_tokens_query=50,\n",
    "    k_tokens_document=192\n",
    ")\n",
    "\n",
    "# Prepare documents to be ingested\n",
    "docs = [\n",
    "    \"Artificial intelligence was founded as an academic discipline in 1956.\",\n",
    "    \"Alan Turing was the first person to conduct substantial research in AI.\",\n",
    "    \"Born in Maida Vale, London, Turing was raised in southern England.\",\n",
    "]\n",
    "\n",
    "# SpladeEmbeddingFunction.encode_documents returns sparse matrix or sparse array depending\n",
    "# on the milvus-model version. reshape(1,-1) ensures the format is correct for ingestion.\n",
    "doc_vector = [{\"embeddings\": doc_emb.reshape(1,-1), \"id\": f\"item_{i}\"} for i, doc_emb in enumerate(embeddings_model.encode_documents(docs))]\n",
    "\n",
    "\n",
    "client.insert(\n",
    "    collection_name=\"my_sparse_collection\",\n",
    "    data=doc_vector\n",
    ")\n",
    "\n",
    "# Prepare search parameters\n",
    "search_params = {\n",
    "    \"params\": {\"drop_ratio_search\": 0.2},  # Additional optional search parameters\n",
    "}\n",
    "\n",
    "# Prepare the query vector\n",
    "\n",
    "queries = [\n",
    "      \"When was artificial intelligence founded\", \n",
    "      \"Where was Turing born?\"\n",
    "]\n",
    "query_vector = embeddings_model.encode_documents(queries)\n",
    "\n",
    "res = client.search(\n",
    "    collection_name=\"my_sparse_collection\",\n",
    "    data=query_vector,\n",
    "    limit=1, #top k documents to return\n",
    "    output_fields=[\"id\"],\n",
    "    search_params=search_params,\n",
    ")\n",
    "\n",
    "for r in res:\n",
    "    print(r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Embeddings using HF Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseSentenceTransformer:\n",
    "    def __init__(self, model_name_or_path, device:str= 'cpu'):\n",
    "        self.model = AutoModelForMaskedLM.from_pretrained(model_name_or_path)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "        self.device = device\n",
    "        self.model.to(device)\n",
    "        if device == \"cuda\":\n",
    "            self.model = self.model.cuda()\n",
    "            self.model = self.model.bfloat16()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode(self, sentences, max_tokens=20):        \n",
    "        if type(sentences) == str:\n",
    "            sentences = [sentences]\n",
    "        \n",
    "        input_dict = self.tokenizer(sentences, max_length=512, padding=True, return_tensors='pt', truncation=True)\n",
    "        attention_mask = input_dict['attention_mask']  # (bs,seqlen)\n",
    "\n",
    "        if self.device == \"cuda\":\n",
    "            input_dict['input_ids'] = input_dict['input_ids'].cuda()\n",
    "            input_dict['attention_mask'] = input_dict['attention_mask'].cuda()\n",
    "            if 'token_type_ids' in input_dict:\n",
    "                input_dict['token_type_ids'] = input_dict['token_type_ids'].cuda()\n",
    "        \n",
    "        hidden_state = self.model(**input_dict)[0]\n",
    "\n",
    "        maxarg = torch.log(1.0 + torch.relu(hidden_state))\n",
    "\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).to(maxarg.device) # bs * seqlen * voc\n",
    "        maxdim1 = torch.max(maxarg * input_mask_expanded, dim=1).values  # bs * voc\n",
    "        \n",
    "        # get topk high weights\n",
    "        topk, indices = torch.topk(maxdim1, k=max_tokens) # (weight - (bs * max_terms), index - (bs * max_terms))\n",
    "        print (topk.shape)\n",
    "\n",
    "        expansions = [[(self.tokenizer.decode(int(indices[sidx][tidx])), float(topk[sidx][tidx])) for tidx in range(topk.shape[1])] for sidx in range(topk.shape[0]) ]  \n",
    "\n",
    "        return expansions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_model = SparseSentenceTransformer(\"ibm-granite/granite-embedding-30m-sparse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[(' AI', 1.667151689529419),\n",
       "  (' intelligence', 1.4905368089675903),\n",
       "  (' artificial', 1.250130534172058),\n",
       "  (' discipline', 1.2192906141281128),\n",
       "  (' founded', 1.0603735446929932),\n",
       "  (' 1956', 1.035099983215332),\n",
       "  (' invention', 0.9785783290863037),\n",
       "  ('56', 0.7224238514900208),\n",
       "  (' learning', 0.6999132037162781),\n",
       "  (' scientific', 0.6892694234848022),\n",
       "  (' computer', 0.6566571593284607),\n",
       "  (' academic', 0.6217383146286011),\n",
       "  (' university', 0.5886250138282776),\n",
       "  (' robot', 0.5613625049591064),\n",
       "  (' establishment', 0.550841748714447),\n",
       "  (' philosophy', 0.5431854128837585),\n",
       "  ('A', 0.5025951862335205),\n",
       "  (' brain', 0.476378858089447),\n",
       "  (' machine', 0.4488101005554199),\n",
       "  ('1960', 0.44649428129196167)]]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change max_tokens to produce more or less expansions for the sentences\n",
    "sparse_model.encode([\"Artificial intelligence was founded as an academic discipline in 1956.\"], max_tokens=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "primeqa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
