{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T22:14:00.547315Z",
     "start_time": "2025-10-02T22:13:56.943733Z"
    }
   },
   "source": [
    "# Load the model\n",
    "from sentence_transformers import SentenceTransformer"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raduf/miniforge3/envs/docu/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T22:18:12.934149Z",
     "start_time": "2025-10-02T22:18:09.629073Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_kwargs={\"attn_implementation\": \"flash_attention_2\"}\n",
    "model = SentenceTransformer('ibm-granite/granite-embedding-english-r2', model_kwargs=model_kwargs)\n",
    "# Encode some text\n",
    "documents = [\n",
    "    \"Granite models are designed for enterprise applications\",\n",
    "    \"Information retrieval systems need fast and accurate embeddings\",\n",
    "    \"Machine learning models can process natural language\"\n",
    "]\n",
    "# Generate embeddings\n",
    "embeddings = model.encode(documents)\n",
    "print(f\"Embedding shape: {embeddings.shape}\")  # (3, 768)"
   ],
   "id": "7d3d4d24b97920e7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Flash Attention 2 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in ModernBertModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", dtype=torch.float16)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: (3, 768)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Generate Text",
   "id": "10e8fdae754b8b29"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T22:18:27.549739Z",
     "start_time": "2025-10-02T22:18:27.547511Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ],
   "id": "d85cf8cd9d1c6e33",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def create_granite4_model():\n",
    "    device = \"cuda\"\n",
    "    granite_model_path = \"ibm-granite/granite-4.0-h-micro\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(granite_model_path)\n",
    "    # drop device_map if running on CPU\n",
    "    granite_model = AutoModelForCausalLM.from_pretrained(granite_model_path, device_map=device)\n",
    "    return granite_model, tokenizer"
   ],
   "id": "4503a65799b5f1e9",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 4,
   "source": "granite_model, tokenizer = create_granite4_model()",
   "id": "f4300d4c09194b2d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T22:20:53.276428Z",
     "start_time": "2025-10-02T22:20:53.273005Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_text(size=1024, num_docs = 1000, device=\"cuda\"):\n",
    "    model.eval()\n",
    "    # change input text as desired\n",
    "    chat = [\n",
    "        { \"role\": \"user\", \"content\": \"Please generate creative text.\" },\n",
    "    ]\n",
    "    chat = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "    # tokenize the text\n",
    "    input_tokens = tokenizer(chat, return_tensors=\"pt\").to(device)\n",
    "    # generate output tokens\n",
    "    output = []\n",
    "    for _ in range(num_docs):\n",
    "        o = granite_model.generate(**input_tokens,\n",
    "                                   max_new_tokens=size)\n",
    "        # decode output tokens into text\n",
    "        o = tokenizer.batch_decode(o)\n",
    "        output.append(o)\n",
    "    # print output\n",
    "    # print(output[0])\n",
    "    return output[0] if len(output) == 1 else output"
   ],
   "id": "e0539992cbdb7f90",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T22:24:12.604970Z",
     "start_time": "2025-10-02T22:20:57.188910Z"
    }
   },
   "cell_type": "code",
   "source": "docs = generate_text(size=512, num_docs=10)",
   "id": "1bc5c692376cbcf3",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T22:25:31.050372Z",
     "start_time": "2025-10-02T22:25:31.047008Z"
    }
   },
   "cell_type": "code",
   "source": "len(docs[0])",
   "id": "d9d145b7de706c94",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T21:28:59.590754Z",
     "start_time": "2025-10-02T21:28:59.587847Z"
    }
   },
   "cell_type": "markdown",
   "source": "Semantic Search Example",
   "id": "689c32f17d1e3acc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T21:32:20.261088Z",
     "start_time": "2025-10-02T21:32:20.259181Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import util"
   ],
   "id": "314555a3b72661f5",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T21:32:34.365740Z",
     "start_time": "2025-10-02T21:32:34.329650Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Encode query and documents\n",
    "query = \"What's the purpose of the granite models?\"\n",
    "query_embedding = model.encode(query)\n",
    "doc_embeddings = model.encode(documents)\n",
    "# Compute cosine similarity\n",
    "similarities = util.cos_sim(query_embedding, doc_embeddings)\n",
    "print(f\"Similarities: {similarities}\")\n",
    "# Get most relevant document\n",
    "best_idx = np.argmax(similarities)\n",
    "print(f\"Most relevant: {documents[best_idx]}\")"
   ],
   "id": "9feaa95672de071a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarities: tensor([[0.8739, 0.7180, 0.7356]])\n",
      "Most relevant: Granite models are designed for enterprise applications\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Building a Semantic Search System\n",
   "id": "adcf4ca15c284d4e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T21:32:55.332648Z",
     "start_time": "2025-10-02T21:32:55.330691Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "import numpy as np"
   ],
   "id": "4dcb58e3dc70d36a",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load retriever and reranker\n",
    "model_kwargs={\"attn_implementation\": \"flash_attention_2\"}\n",
    "retriever = SentenceTransformer('ibm-granite/granite-embedding-english-r2',\n",
    "                                model_kwargs=model_kwargs)\n",
    "reranker = CrossEncoder('ibm-granite/granite-embedding-reranker-english-r2',\n",
    "                        model_kwargs=model_kwargs)\n",
    "# Your document corpus\n",
    "corpus = [\n",
    "    \"Python is a high-level programming language\",\n",
    "    \"Machine learning models require training data\",\n",
    "    \"Natural language processing enables text understanding\",\n",
    "    \"Deep learning uses neural networks with multiple layers\",\n",
    "    \"Data science combines statistics and programming\",\n",
    "    # ... your documents here\n",
    "]\n",
    "# Step 1: Encode corpus once (can be cached)\n",
    "corpus_embeddings = retriever.encode(corpus, convert_to_tensor=True)\n",
    "# Step 2: Retrieve top-k candidates\n",
    "def search(query, top_k=20):\n",
    "    query_embedding = retriever.encode(query, convert_to_tensor=True)\n",
    "\n",
    "    # Find top-k with retriever\n",
    "    hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=top_k)[0]\n",
    "\n",
    "    # Step 3: Rerank with cross-encoder\n",
    "    cross_inp = [[query, corpus[hit['corpus_id']]] for hit in hits]\n",
    "    cross_scores = reranker.predict(cross_inp)\n",
    "\n",
    "    # Sort by reranker scores\n",
    "    for idx, score in enumerate(cross_scores):\n",
    "        hits[idx]['rerank_score'] = score\n",
    "\n",
    "    hits = sorted(hits, key=lambda x: x['rerank_score'], reverse=True)\n",
    "\n",
    "    return hits[:5]  # Return top 5 after reranking\n",
    "# Use it\n",
    "results = search(\"What is machine learning?\")\n",
    "for hit in results:\n",
    "    print(f\"Score: {hit['rerank_score']:.4f} | {corpus[hit['corpus_id']]}\")"
   ],
   "id": "cd5ff20b34def5f",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 1.0000 | Machine learning models require training data\n",
      "Score: 1.0000 | Data science combines statistics and programming\n",
      "Score: 1.0000 | Deep learning uses neural networks with multiple layers\n",
      "Score: 1.0000 | Natural language processing enables text understanding\n",
      "Score: 1.0000 | Python is a high-level programming language\n"
     ]
    }
   ],
   "execution_count": 25,
   "source": "Long Context",
   "id": "e756221ab794dc47"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Load model with long context support\n",
    "model = SentenceTransformer('ibm-granite/granite-embedding-english-r2')"
   ],
   "id": "34d99f191bf50995",
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T21:40:19.095192Z",
     "start_time": "2025-10-02T21:40:19.020504Z"
    }
   },
   "cell_type": "code",
   "source": [
    "long_document=\"\"\"\n",
    "IBM Research introduces next-generation embedding models that don’t compromise between speed and accuracy\n",
    "\n",
    "When it comes to enterprise information retrieval, organizations face a persistent challenge: existing embedding models force you to choose between accuracy and speed, between long-context support and commercial licensing, between general-purpose performance and domain-specific excellence.\n",
    "\n",
    "On August 15th , we’ve introduced the Granite Embedding R2 models — a comprehensive family of retrieval models designed to eliminate these tradeoffs.\n",
    "\n",
    "What’s New in R2?\n",
    "The Granite Embedding R2 release includes three models, all available under Apache 2.0 license:\n",
    "\n",
    "granite-embedding-english-r2 (149M parameters): Our flagship model with 768-dimensional embeddings\n",
    "granite-embedding-small-english-r2 (47M parameters): A first-of-its-kind efficient model with 384-dimensional embeddings\n",
    "granite-embedding-reranker-english-r2 (149M parameters): A cross-encoder for precision ranking\n",
    "These models deliver three critical improvements over our first-generation release:\n",
    "\n",
    "16x expanded context length from 512 to 8,192 tokens — meeting modern document processing requirements\n",
    "19–44% faster inference than comparable models, without sacrificing accuracy\n",
    "State-of-the-art performance across text, code, long-documents, conversational queries, and tabular data\n",
    "Getting Started: Basic Usage\n",
    "Using Granite Embedding models is straightforward with the Sentence-Transformers library:\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "# Load the model\n",
    "model = SentenceTransformer('ibm-granite/granite-embedding-english-r2')\n",
    "# Encode some text\n",
    "documents = [\n",
    "    \"Granite models are designed for enterprise applications\",\n",
    "    \"Information retrieval systems need fast and accurate embeddings\",\n",
    "    \"Machine learning models can process natural language\"\n",
    "]\n",
    "# Generate embeddings\n",
    "embeddings = model.encode(documents)\n",
    "print(f\"Embedding shape: {embeddings.shape}\")  # (3, 768)\n",
    "For semantic search, you can compute similarity scores:\n",
    "\n",
    "import numpy as np\n",
    "from sentence_transformers import util\n",
    "# Encode query and documents\n",
    "query = \"What are enterprise AI models?\"\n",
    "query_embedding = model.encode(query)\n",
    "doc_embeddings = model.encode(documents)\n",
    "# Compute cosine similarity\n",
    "similarities = util.cos_sim(query_embedding, doc_embeddings)\n",
    "print(f\"Similarities: {similarities}\")\n",
    "# Get most relevant document\n",
    "best_idx = np.argmax(similarities)\n",
    "print(f\"Most relevant: {documents[best_idx]}\")\n",
    "Building a Semantic Search System\n",
    "Here’s a complete example building a retrieval system with the reranker:\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "import numpy as np\n",
    "# Load retriever and reranker\n",
    "retriever = SentenceTransformer('ibm-granite/granite-embedding-english-r2')\n",
    "reranker = CrossEncoder('ibm-granite/granite-embedding-reranker-english-r2')\n",
    "# Your document corpus\n",
    "corpus = [\n",
    "    \"Python is a high-level programming language\",\n",
    "    \"Machine learning models require training data\",\n",
    "    \"Natural language processing enables text understanding\",\n",
    "    \"Deep learning uses neural networks with multiple layers\",\n",
    "    \"Data science combines statistics and programming\",\n",
    "    # ... your documents here\n",
    "]\n",
    "# Step 1: Encode corpus once (can be cached)\n",
    "corpus_embeddings = retriever.encode(corpus, convert_to_tensor=True)\n",
    "# Step 2: Retrieve top-k candidates\n",
    "def search(query, top_k=20):\n",
    "    query_embedding = retriever.encode(query, convert_to_tensor=True)\n",
    "\n",
    "    # Find top-k with retriever\n",
    "    hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=top_k)[0]\n",
    "\n",
    "    # Step 3: Rerank with cross-encoder\n",
    "    cross_inp = [[query, corpus[hit['corpus_id']]] for hit in hits]\n",
    "    cross_scores = reranker.predict(cross_inp)\n",
    "\n",
    "    # Sort by reranker scores\n",
    "    for idx, score in enumerate(cross_scores):\n",
    "        hits[idx]['rerank_score'] = score\n",
    "\n",
    "    hits = sorted(hits, key=lambda x: x['rerank_score'], reverse=True)\n",
    "\n",
    "    return hits[:5]  # Return top 5 after reranking\n",
    "# Use it\n",
    "results = search(\"What is machine learning?\")\n",
    "for hit in results:\n",
    "    print(f\"Score: {hit['rerank_score']:.4f} | {corpus[hit['corpus_id']]}\")\n",
    "Long Context Documents\n",
    "Granite R2 handles up to 8,192 tokens, perfect for processing full documents:\n",
    "\n",
    "# Load model with long context support\n",
    "model = SentenceTransformer('ibm-granite/granite-embedding-english-r2')\n",
    "# Process a long document (e.g., research paper, technical documentation)\n",
    "long_document =\n",
    "[Your 5000+ word document here]\n",
    "This could be an entire research paper, technical manual,\n",
    "or any long-form content...\n",
    "\n",
    "# Encode the full document (no chunking needed for <8192 tokens)\n",
    "doc_embedding = model.encode(long_document, show_progress_bar=True)\n",
    "# Compare with shorter query\n",
    "query = \"What are the main findings of this research?\"\n",
    "query_embedding = model.encode(query)\n",
    "similarity = util.cos_sim(query_embedding, doc_embedding)\n",
    "print(f\"Relevance score: {similarity.item():.4f}\")\n",
    "Code Search Example\n",
    "Granite R2 excels at code retrieval:\n",
    "\n",
    "# Code snippets corpus\n",
    "code_snippets = [\n",
    "\n",
    "    def binary_search(arr, target):\n",
    "        left, right = 0, len(arr) - 1\n",
    "        while left <= right:\n",
    "            mid = (left + right) // 2\n",
    "            if arr[mid] == target:\n",
    "                return mid\n",
    "            elif arr[mid] < target:\n",
    "                left = mid + 1\n",
    "            else:\n",
    "                right = mid - 1\n",
    "        return -1\n",
    "\n",
    "    def quicksort(arr):\n",
    "        if len(arr) <= 1:\n",
    "            return arr\n",
    "        pivot = arr[len(arr) // 2]\n",
    "        left = [x for x in arr if x < pivot]\n",
    "        middle = [x for x in arr if x == pivot]\n",
    "        right = [x for x in arr if x > pivot]\n",
    "        return quicksort(left) + middle + quicksort(right)\n",
    "    class LinkedList:\n",
    "        def __init__(self):\n",
    "            self.head = None\n",
    "\n",
    "        def append(self, data):\n",
    "            if not self.head:\n",
    "                self.head = Node(data)\n",
    "                return\n",
    "            current = self.head\n",
    "            while current.next:\n",
    "                current = current.next\n",
    "            current.next = Node(data)\n",
    "]\n",
    "# Encode code\n",
    "code_embeddings = model.encode(code_snippets)\n",
    "# Natural language query\n",
    "query = \"How do I implement a binary search algorithm?\"\n",
    "query_embedding = model.encode(query)\n",
    "# Find most relevant code\n",
    "similarities = util.cos_sim(query_embedding, code_embeddings)[0]\n",
    "best_match = np.argmax(similarities)\n",
    "print(f\"Most relevant code snippet:\\n{code_snippets[best_match]}\")\n",
    "Table Retrieval\n",
    "Handle structured data with ease:\n",
    "\n",
    "# Tables in markdown format\n",
    "tables = [\n",
    "    | Product | Q1 Revenue | Q2 Revenue |\n",
    "    |---------|-----------|-----------|\n",
    "    | Product A | $500K | $650K |\n",
    "    | Product B | $300K | $420K |\n",
    "    | Employee | Department | Salary |\n",
    "    |----------|-----------|--------|\n",
    "    | John Doe | Engineering | $120K |\n",
    "    | Jane Smith | Marketing | $95K |\n",
    "    | Country | Population | GDP |\n",
    "    |---------|-----------|-----|\n",
    "    | USA | 331M | $21T |\n",
    "    | China | 1.4B | $14T |\n",
    "\n",
    "]\n",
    "# Encode tables\n",
    "table_embeddings = model.encode(tables)\n",
    "# Query for specific information\n",
    "query = \"What was the revenue growth for our products?\"\n",
    "query_embedding = model.encode(query)\n",
    "similarities = util.cos_sim(query_embedding, table_embeddings)[0]\n",
    "best_table = np.argmax(similarities)\n",
    "print(f\"Most relevant table:\\n{tables[best_table]}\")\n",
    "Batch Processing for Production\n",
    "For production deployments processing large volumes:\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "# Load model with GPU support\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = SentenceTransformer('ibm-granite/granite-embedding-english-r2', device=device)\n",
    "# Large batch of documents\n",
    "documents = [...] # Your thousands of documents\n",
    "# Efficient batch encoding\n",
    "batch_size = 128\n",
    "all_embeddings = model.encode(\n",
    "    documents,\n",
    "    batch_size=batch_size,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_tensor=True,\n",
    "    normalize_embeddings=True  # For cosine similarity\n",
    ")\n",
    "# Save embeddings for later use\n",
    "torch.save(all_embeddings, 'document_embeddings.pt')\n",
    "# Load and search later\n",
    "embeddings = torch.load('document_embeddings.pt')\n",
    "query_emb = model.encode(query, convert_to_tensor=True)\n",
    "hits = util.semantic_search(query_emb, embeddings, top_k=10)\n",
    "Multi-Turn Conversational Search\n",
    "Handle conversational context:\n",
    "\n",
    "conversation_history = [\n",
    "    \"What are the system requirements for the new software?\",\n",
    "    \"Does it work on Mac?\",\n",
    "    \"What about Linux distributions?\"\n",
    "]\n",
    "# Concatenate conversation context\n",
    "context = \" \".join(conversation_history)\n",
    "current_query = conversation_history[-1]\n",
    "# Encode with full context\n",
    "context_embedding = model.encode(context)\n",
    "# Search in your knowledge base\n",
    "results = search_with_context(context_embedding, knowledge_base)\n",
    "Built on Modern Foundations\n",
    "The R2 models leverage the ModernBERT architecture, incorporating recent advances in encoder design:\n",
    "\n",
    "Alternating attention mechanisms for efficiency\n",
    "Rotary positional embeddings enabling flexible context lengths\n",
    "Flash Attention support for optimized inference\n",
    "We trained these models on 2 trillion tokens from high-quality sources including GneissWeb, Wikipedia, and Granite Code data. Every dataset underwent comprehensive governance review, with screening for personal information and profanity — because enterprise deployments demand transparency and responsible AI practices.\n",
    "\n",
    "A Novel Training Pipeline\n",
    "What sets Granite R2 apart is our five-stage training methodology:\n",
    "\n",
    "1. Retrieval-Oriented Pretraining: Using RetroMAE to train rich [CLS] representations without explicit contrastive objectives\n",
    "\n",
    "2. Tabular Pretraining: A breakthrough approach for handling structured data. Traditional embedding models struggle with tables containing numerical data and limited context. Our solution? We generated synthetic summaries for 8 million tables using Mistral-7B, then modified the RetroMAE objective to predict masked tokens over summaries rather than table content itself. This forces the encoder to align table structure with natural language descriptions.\n",
    "\n",
    "3. Contrastive Finetuning: Training on large-scale semi-supervised pairs with improved contrastive loss\n",
    "\n",
    "4. Contrastive Distillation: Rather than simply finetuning on hard negatives, we distill knowledge from a Mistral-7B teacher model trained on high-quality triples. This approach yields larger performance gains than traditional hard-negative training.\n",
    "\n",
    "5. Domain Adaptation: Specialized training for multi-turn conversational retrieval\n",
    "\n",
    "This pipeline enables a single model family to excel across remarkably diverse tasks.\n",
    "\n",
    "Performance That Speaks for Itself\n",
    "We evaluated Granite R2 across comprehensive benchmarks:\n",
    "\n",
    "General Retrieval (MTEB-v2)\n",
    "\n",
    "granite-english-r2: 56.4 average score\n",
    "granite-small-r2: 53.9 average score\n",
    "Code Retrieval (COIR)\n",
    "\n",
    "54.8 / 53.4 across text-to-code, code-to-text, and hybrid tasks\n",
    "Zero-shot evaluation (no COIR training data used)\n",
    "Long-Context (MLDR, LongEmbed)\n",
    "\n",
    "granite-english-r2: 41.6 MLDR, 67.8 LongEmbed\n",
    "granite-small-r2: 40.1 MLDR, 61.9 LongEmbed\n",
    "State-of-the-art on LongEmbed benchmark\n",
    "Table Retrieval\n",
    "\n",
    "78.5 / 75.5 across OpenWikiTables, NQTables, OTT-QA, MultiHierTT, and AIT-QA\n",
    "Multi-Turn Conversational (MT-RAG)\n",
    "\n",
    "granite-english-r2: 57.6 Recall@5\n",
    "Substantial improvement over first-generation models\n",
    "Speed Without Compromise\n",
    "Performance benchmarks often overlook a critical real-world constraint: encoding speed. When you’re ingesting millions of documents with frequent updates, speed directly impacts operational costs and user experience.\n",
    "\n",
    "We benchmarked encoding speed using 23,000 IBM technical documents (averaging 6,393 characters, ranging from 10 to 475,001 characters):\n",
    "\n",
    "granite-english-r2: 144 documents/second\n",
    "granite-small-r2: 199 documents/second\n",
    "These speeds represent 19–44% improvements over leading competitors, despite the R2 models having slightly more parameters than R1. The ModernBERT architecture’s optimizations — particularly Flash Attention — enable this efficiency gain.\n",
    "\n",
    "Complete Retrieval Ecosystem\n",
    "The reranker model completes the retrieval pipeline. Built on granite-embedding-english-r2, it uses a PListMLE loss objective for position-aware ranking:\n",
    "\n",
    "BEIR: 55.4 (vs. 53.1 for retriever alone)\n",
    "MLDR: 44.4 (vs. 41.6 for retriever alone)\n",
    "This retrieve-and-rerank framework maximizes both recall and precision without severe computational overhead.\n",
    "\n",
    "Enterprise-Ready from Day One\n",
    "Every Granite model prioritizes enterprise requirements:\n",
    "\n",
    "Data Governance: Comprehensive clearance process capturing content description, intended use, data classification, licensing information, usage restrictions, and personal information assessment\n",
    "\n",
    "Licensing: Apache 2.0 — no restrictions on commercial use, no proprietary training data limitations\n",
    "\n",
    "Transparency: Fully documented training data sources, architectural decisions, and evaluation methodology\n",
    "\n",
    "Why This Matters\n",
    "Information retrieval isn’t just about finding documents — it’s about enabling AI systems to access relevant knowledge efficiently. Whether you’re building RAG applications, semantic search engines, or recommendation systems, embedding quality and speed determine what’s possible.\n",
    "\n",
    "Granite R2 models don’t force you to choose between accuracy and speed, between long-context support and efficiency, between general-purpose capability and domain-specific performance. They deliver all of it.\n",
    "\n",
    "In an era where milliseconds matter and accuracy cannot be compromised, Granite R2 doesn’t just meet the standard — it sets it.\n",
    "\n",
    "Get Started\n",
    "All Granite Embedding R2 models are available now on Hugging Face under Apache 2.0 license:\n",
    "\n",
    "granite-embedding-english-r2\n",
    "granite-embedding-small-english-r2\n",
    "granite-embedding-reranker-english-r2\n",
    "For technical details, architecture description, and comprehensive benchmark results, see our research paper.\n",
    "\n",
    "The Granite Embedding R2 models represent collaborative work across IBM Research teams in multiple geographies. For questions or feedback, visit our GitHub repository.\n",
    "\"\"\"\n",
    "# Encode the full document (no chunking needed for <8192 tokens)\n",
    "doc_embedding = model.encode(long_document, show_progress_bar=True)\n",
    "# Compare with shorter query\n",
    "query = \"What are the main findings of this research?\"\n",
    "query_embedding = model.encode(query)\n",
    "similarity = util.cos_sim(query_embedding, doc_embedding)\n",
    "print(f\"Relevance score: {similarity.item():.4f}\")"
   ],
   "id": "4b3d9c7247e533cc",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 22.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevance score: 0.7477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Code Search Example",
   "id": "70401c76b2c4ab34"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T21:41:03.655344Z",
     "start_time": "2025-10-02T21:41:03.652918Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Code snippets corpus\n",
    "code_snippets = [\n",
    "    \"\"\"\n",
    "    def binary_search(arr, target):\n",
    "        left, right = 0, len(arr) - 1\n",
    "        while left <= right:\n",
    "            mid = (left + right) // 2\n",
    "            if arr[mid] == target:\n",
    "                return mid\n",
    "            elif arr[mid] < target:\n",
    "                left = mid + 1\n",
    "            else:\n",
    "                right = mid - 1\n",
    "        return -1\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    def quicksort(arr):\n",
    "        if len(arr) <= 1:\n",
    "            return arr\n",
    "        pivot = arr[len(arr) // 2]\n",
    "        left = [x for x in arr if x < pivot]\n",
    "        middle = [x for x in arr if x == pivot]\n",
    "        right = [x for x in arr if x > pivot]\n",
    "        return quicksort(left) + middle + quicksort(right)\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    class LinkedList:\n",
    "        def __init__(self):\n",
    "            self.head = None\n",
    "\n",
    "        def append(self, data):\n",
    "            if not self.head:\n",
    "                self.head = Node(data)\n",
    "                return\n",
    "            current = self.head\n",
    "            while current.next:\n",
    "                current = current.next\n",
    "            current.next = Node(data)\n",
    "    \"\"\"\n",
    "]"
   ],
   "id": "1e2d958c8c7d1353",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T21:42:57.281638Z",
     "start_time": "2025-10-02T21:42:57.238468Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Encode code\n",
    "code_embeddings = model.encode(code_snippets)\n",
    "# Natural language query\n",
    "query = \"How do I implement a binary search algorithm?\"\n",
    "query_embedding = model.encode(query)\n",
    "# Find most relevant code\n",
    "similarities = util.cos_sim(query_embedding, code_embeddings)[0]\n",
    "best_match = np.argmax(similarities)\n",
    "print(f\"Most relevant code snippet:\\n{code_snippets[best_match]}\")\n"
   ],
   "id": "1aac12e98f98fd49",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most relevant code snippet:\n",
      "\n",
      "    class LinkedList:\n",
      "        def __init__(self):\n",
      "            self.head = None\n",
      "\n",
      "        def append(self, data):\n",
      "            if not self.head:\n",
      "                self.head = Node(data)\n",
      "                return\n",
      "            current = self.head\n",
      "            while current.next:\n",
      "                current = current.next\n",
      "            current.next = Node(data)\n",
      "    \n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T21:43:10.456758Z",
     "start_time": "2025-10-02T21:43:10.453452Z"
    }
   },
   "cell_type": "markdown",
   "source": "Table Retrieval",
   "id": "bf6f91051e61c4ce"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T21:43:30.842530Z",
     "start_time": "2025-10-02T21:43:30.840175Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Tables in markdown format\n",
    "tables = [\n",
    "    \"\"\"\n",
    "    | Product | Q1 Revenue | Q2 Revenue |\n",
    "    |---------|-----------|-----------|\n",
    "    | Product A | $500K | $650K |\n",
    "    | Product B | $300K | $420K |\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    | Employee | Department | Salary |\n",
    "    |----------|-----------|--------|\n",
    "    | John Doe | Engineering | $120K |\n",
    "    | Jane Smith | Marketing | $95K |\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    | Country | Population | GDP |\n",
    "    |---------|-----------|-----|\n",
    "    | USA | 331M | $21T |\n",
    "    | China | 1.4B | $14T |\n",
    "    \"\"\"\n",
    "]"
   ],
   "id": "42f471d78807a1f6",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T21:44:04.856174Z",
     "start_time": "2025-10-02T21:44:04.809768Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Encode tables\n",
    "table_embeddings = model.encode(tables)\n",
    "# Query for specific information\n",
    "query = \"What was the revenue growth for our products?\"\n",
    "query_embedding = model.encode(query)\n",
    "similarities = util.cos_sim(query_embedding, table_embeddings)[0]\n",
    "best_table = np.argmax(similarities)\n",
    "print(f\"Most relevant table:\\n{tables[best_table]}\")"
   ],
   "id": "5c89e8b352a61bc7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most relevant table:\n",
      "\n",
      "    | Product | Q1 Revenue | Q2 Revenue |\n",
      "    |---------|-----------|-----------|\n",
      "    | Product A | $500K | $650K |\n",
      "    | Product B | $300K | $420K |\n",
      "    \n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T21:44:12.194336Z",
     "start_time": "2025-10-02T21:44:12.191920Z"
    }
   },
   "cell_type": "markdown",
   "source": "Batch Processing for Production",
   "id": "efa52385ce012d59"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T21:44:24.564430Z",
     "start_time": "2025-10-02T21:44:24.562222Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch"
   ],
   "id": "97276dfd4353655d",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T21:44:50.125800Z",
     "start_time": "2025-10-02T21:44:48.633184Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load model with GPU support\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = SentenceTransformer('ibm-granite/granite-embedding-english-r2', device=device,\n",
    "                            model_kwargs=model_kwargs)\n",
    "# Large batch of documents\n",
    "# documents = [...] # Your thousands of documents\n",
    "documents = generate_text(1024, num_docs=128)\n",
    "# Efficient batch encoding\n",
    "batch_size = 64\n",
    "all_embeddings = model.encode_document(\n",
    "    documents,\n",
    "    batch_size=batch_size,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_tensor=True,\n",
    "    normalize_embeddings=True  # For cosine similarity\n",
    ")\n",
    "# Save embeddings for later use\n",
    "torch.save(all_embeddings, 'document_embeddings.pt')\n",
    "# Load and search later\n",
    "embeddings = torch.load('document_embeddings.pt')\n",
    "query_emb = model.encode(query, convert_to_tensor=True)\n",
    "hits = util.semantic_search(query_emb, embeddings, top_k=10)"
   ],
   "id": "98d40bbf0f019dfe",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches:   0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'ellipsis' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mTypeError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniforge3/envs/docu/lib/python3.12/site-packages/sentence_transformers/SentenceTransformer.py:1621\u001B[39m, in \u001B[36mSentenceTransformer.tokenize\u001B[39m\u001B[34m(self, texts, **kwargs)\u001B[39m\n\u001B[32m   1620\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1621\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m[\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtokenize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtexts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1622\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniforge3/envs/docu/lib/python3.12/site-packages/sentence_transformers/models/Transformer.py:307\u001B[39m, in \u001B[36mTransformer.tokenize\u001B[39m\u001B[34m(self, texts, padding)\u001B[39m\n\u001B[32m    306\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m text_tuple \u001B[38;5;129;01min\u001B[39;00m texts:\n\u001B[32m--> \u001B[39m\u001B[32m307\u001B[39m     batch1.append(\u001B[43mtext_tuple\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m]\u001B[49m)\n\u001B[32m    308\u001B[39m     batch2.append(text_tuple[\u001B[32m1\u001B[39m])\n",
      "\u001B[31mTypeError\u001B[39m: 'ellipsis' object is not subscriptable",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[31mTypeError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[44]\u001B[39m\u001B[32m, line 9\u001B[39m\n\u001B[32m      7\u001B[39m \u001B[38;5;66;03m# Efficient batch encoding\u001B[39;00m\n\u001B[32m      8\u001B[39m batch_size = \u001B[32m128\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m9\u001B[39m all_embeddings = \u001B[43mmodel\u001B[49m\u001B[43m.\u001B[49m\u001B[43mencode\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     10\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdocuments\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     11\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     12\u001B[39m \u001B[43m    \u001B[49m\u001B[43mshow_progress_bar\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m     13\u001B[39m \u001B[43m    \u001B[49m\u001B[43mconvert_to_tensor\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m     14\u001B[39m \u001B[43m    \u001B[49m\u001B[43mnormalize_embeddings\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# For cosine similarity\u001B[39;49;00m\n\u001B[32m     15\u001B[39m \u001B[43m)\u001B[49m\n\u001B[32m     16\u001B[39m \u001B[38;5;66;03m# Save embeddings for later use\u001B[39;00m\n\u001B[32m     17\u001B[39m torch.save(all_embeddings, \u001B[33m'\u001B[39m\u001B[33mdocument_embeddings.pt\u001B[39m\u001B[33m'\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniforge3/envs/docu/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001B[39m, in \u001B[36mcontext_decorator.<locals>.decorate_context\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    117\u001B[39m \u001B[38;5;129m@functools\u001B[39m.wraps(func)\n\u001B[32m    118\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mdecorate_context\u001B[39m(*args, **kwargs):\n\u001B[32m    119\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[32m--> \u001B[39m\u001B[32m120\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniforge3/envs/docu/lib/python3.12/site-packages/sentence_transformers/SentenceTransformer.py:1062\u001B[39m, in \u001B[36mSentenceTransformer.encode\u001B[39m\u001B[34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, truncate_dim, pool, chunk_size, **kwargs)\u001B[39m\n\u001B[32m   1060\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m start_index \u001B[38;5;129;01min\u001B[39;00m trange(\u001B[32m0\u001B[39m, \u001B[38;5;28mlen\u001B[39m(sentences), batch_size, desc=\u001B[33m\"\u001B[39m\u001B[33mBatches\u001B[39m\u001B[33m\"\u001B[39m, disable=\u001B[38;5;129;01mnot\u001B[39;00m show_progress_bar):\n\u001B[32m   1061\u001B[39m     sentences_batch = sentences_sorted[start_index : start_index + batch_size]\n\u001B[32m-> \u001B[39m\u001B[32m1062\u001B[39m     features = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtokenize\u001B[49m\u001B[43m(\u001B[49m\u001B[43msentences_batch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1063\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.device.type == \u001B[33m\"\u001B[39m\u001B[33mhpu\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m   1064\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33minput_ids\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m features:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniforge3/envs/docu/lib/python3.12/site-packages/sentence_transformers/SentenceTransformer.py:1623\u001B[39m, in \u001B[36mSentenceTransformer.tokenize\u001B[39m\u001B[34m(self, texts, **kwargs)\u001B[39m\n\u001B[32m   1621\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m[\u001B[32m0\u001B[39m].tokenize(texts, **kwargs)\n\u001B[32m   1622\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1623\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m[\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtokenize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtexts\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniforge3/envs/docu/lib/python3.12/site-packages/sentence_transformers/models/Transformer.py:307\u001B[39m, in \u001B[36mTransformer.tokenize\u001B[39m\u001B[34m(self, texts, padding)\u001B[39m\n\u001B[32m    305\u001B[39m batch1, batch2 = [], []\n\u001B[32m    306\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m text_tuple \u001B[38;5;129;01min\u001B[39;00m texts:\n\u001B[32m--> \u001B[39m\u001B[32m307\u001B[39m     batch1.append(\u001B[43mtext_tuple\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m]\u001B[49m)\n\u001B[32m    308\u001B[39m     batch2.append(text_tuple[\u001B[32m1\u001B[39m])\n\u001B[32m    309\u001B[39m to_tokenize = [batch1, batch2]\n",
      "\u001B[31mTypeError\u001B[39m: 'ellipsis' object is not subscriptable"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5759c373c033b1a6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
