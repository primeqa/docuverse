{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T17:53:16.696242Z",
     "start_time": "2025-10-08T17:53:16.693206Z"
    }
   },
   "source": [
    "# Load the model\n",
    "from sentence_transformers import SentenceTransformer, util"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T16:19:44.340994Z",
     "start_time": "2025-10-08T16:19:40.884735Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "model_kwargs={\"attn_implementation\": \"flash_attention_2\",\n",
    "              \"dtype\": torch.bfloat16}\n",
    "model = SentenceTransformer('ibm-granite/granite-embedding-english-r2',\n",
    "                            model_kwargs=model_kwargs)\n",
    "# Encode some text\n",
    "documents = [\n",
    "    \"Granite models are designed for enterprise applications\",\n",
    "    \"Information retrieval systems need fast and accurate embeddings\",\n",
    "    \"Machine learning models can process natural language\"\n",
    "]\n",
    "# Generate embeddings\n",
    "embeddings = model.encode(documents)\n",
    "print(f\"Embedding shape: {embeddings.shape}\")  # (3, 768)"
   ],
   "id": "7d3d4d24b97920e7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: (3, 768)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Generate Text\n",
    "\n",
    "This section provides the tools to generate text samples using the Granite language model. Feel free to customize the prompts and parameters to generate your own text content.\n"
   ],
   "id": "f61971effddd032"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T22:09:59.665092Z",
     "start_time": "2025-10-07T22:09:59.662722Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ],
   "id": "d85cf8cd9d1c6e33",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T22:12:50.237425Z",
     "start_time": "2025-10-07T22:12:50.235274Z"
    }
   },
   "cell_type": "code",
   "source": [
    "default_llm = \"ibm-granite/granite-4.0-h-micro\"\n",
    "def create_granite4_model(llm_to_use):\n",
    "    device = \"cuda\"\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(llm_to_use)\n",
    "    # drop device_map if running on CPU\n",
    "    granite_model = AutoModelForCausalLM.from_pretrained(llm_to_use,\n",
    "                                                         device_map=device)\n",
    "    return granite_model, tokenizer"
   ],
   "id": "4503a65799b5f1e9",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T22:12:56.279546Z",
     "start_time": "2025-10-07T22:12:52.844740Z"
    }
   },
   "cell_type": "code",
   "source": "granite_model, tokenizer = create_granite4_model(default_llm)",
   "id": "f4300d4c09194b2d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The fast path is not available because on of `(selective_state_update, causal_conv1d_fn, causal_conv1d_update)` is None. Falling back to the naive implementation. To install follow https://github.com/state-spaces/mamba/#installation and https://github.com/Dao-AILab/causal-conv1d\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ed53fb975aa942078ca48887c0bea55a"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T22:33:35.893357Z",
     "start_time": "2025-10-07T22:33:35.889322Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm.auto import tqdm\n",
    "def generate_text(size=1024, num_docs = 1000, device=\"cuda\"):\n",
    "    model.eval()\n",
    "    # change input text as desired\n",
    "    chat = [\n",
    "        { \"role\": \"user\", \"content\": \"Please generate creative text.\" },\n",
    "    ]\n",
    "    chat = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "    # tokenize the text\n",
    "    input_tokens = tokenizer(chat, return_tensors=\"pt\").to(device)\n",
    "    # generate output tokens\n",
    "    output = []\n",
    "    for _ in tqdm(range(num_docs), desc=\"Generating text\"):\n",
    "        o = granite_model.generate(**input_tokens,\n",
    "                                   max_new_tokens=size)\n",
    "        # decode output tokens into text\n",
    "        response_text = tokenizer.batch_decode(o, skip_special_tokens=False)[0]\n",
    "        assistant_turn_marker = \"<|start_of_role|>assistant\"\n",
    "        if assistant_turn_marker in response_text:\n",
    "            # Get the text after the last assistant turn marker\n",
    "            new_assistant_turn = response_text.rsplit(assistant_turn_marker, 1)[-1].strip()\n",
    "            # Clean up any remaining tokens or unwanted text\n",
    "            final_response = new_assistant_turn.replace(\"<|end_of_role|>\", \"\").strip()\n",
    "        else:\n",
    "            final_response = response_text.strip()\n",
    "        output.append(final_response)\n",
    "    # print output\n",
    "    # print(output[0])\n",
    "    return output[0] if len(output) == 1 else output"
   ],
   "id": "e0539992cbdb7f90",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-07T22:35:20.934215Z",
     "start_time": "2025-10-07T22:34:28.168002Z"
    }
   },
   "cell_type": "code",
   "source": "docs = generate_text(size=128, num_docs=10)",
   "id": "1bc5c692376cbcf3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generating text:   0%|          | 0/10 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0ae5892d2e114deaa563f6811841f946"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "docs[1]==docs[0]",
   "id": "d9d145b7de706c94",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Generate text using LMStudio",
   "id": "e27b7ba3b59a7253"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!pip install openai",
   "id": "676eda996e4f0364",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T16:22:01.044088Z",
     "start_time": "2025-10-08T16:22:01.034454Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from openai import OpenAI\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Point to your local LM Studio server\n",
    "client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"not-needed\")"
   ],
   "id": "7bd13df98876d772",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T16:29:58.514950Z",
     "start_time": "2025-10-08T16:29:58.511687Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_text_lmstudio(size=1024, num_docs=1000):\n",
    "    # generate output tokens\n",
    "    output = []\n",
    "    for _ in tqdm(range(num_docs), desc=\"Generating text\"):\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"local-model\",  # LM Studio ignores this, uses loaded model\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a creative assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Generate an interesting story of at most {size} tokens.\"}\n",
    "            ],\n",
    "            temperature=0.7,\n",
    "            max_tokens=size\n",
    "        )\n",
    "\n",
    "        output.append(response.choices[0].message.content)\n",
    "    return output\n",
    "    # print(output[0])"
   ],
   "id": "a428ed5d74c5800",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T16:28:47.398325Z",
     "start_time": "2025-10-08T16:28:07.348551Z"
    }
   },
   "cell_type": "code",
   "source": "docs = generate_text_lmstudio(size=512, num_docs=10)",
   "id": "73f9bd544f90f81b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generating text:   0%|          | 0/10 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ac006f796bfe47e1b30574a725957f11"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T21:28:59.590754Z",
     "start_time": "2025-10-02T21:28:59.587847Z"
    }
   },
   "cell_type": "markdown",
   "source": [
    "### Semantic Search Example\n",
    "This is how you can easily set up a similarity computation with the granite embedding models."
   ],
   "id": "689c32f17d1e3acc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T16:30:21.983117Z",
     "start_time": "2025-10-08T16:30:21.981218Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import util"
   ],
   "id": "314555a3b72661f5",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T16:31:15.780750Z",
     "start_time": "2025-10-08T16:31:15.733354Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Encode query and documents\n",
    "query = \"What's the purpose of the granite models?\"\n",
    "query_embedding = model.encode(query)\n",
    "doc_embeddings = model.encode(documents)\n",
    "# Compute cosine similarity\n",
    "similarities = util.cos_sim(query_embedding, doc_embeddings)\n",
    "print(f\"Similarities: {similarities}\")\n",
    "# Get most relevant document\n",
    "best_idx = np.argmax(similarities)\n",
    "print(f\"Most relevant: {documents[best_idx]}\")"
   ],
   "id": "9feaa95672de071a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarities: tensor([[0.8735, 0.7191, 0.7353]])\n",
      "Most relevant: Granite models are designed for enterprise applications\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Building a Semantic Search System\n",
   "id": "adcf4ca15c284d4e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T17:53:25.956381Z",
     "start_time": "2025-10-08T17:53:25.954441Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "import numpy as np"
   ],
   "id": "4dcb58e3dc70d36a",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T17:58:53.000554Z",
     "start_time": "2025-10-08T17:58:52.983037Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sentence_transformers import SentenceTransformer, CrossEncoder, util\n",
    "import numpy as np\n",
    "\n",
    "def create_search_system(retriever_name='ibm-granite/granite-embedding-english-r2',\n",
    "                         reranker_name='ibm-granite/granite-embedding-reranker-english-r2'):\n",
    "    \"\"\"\n",
    "    Create a search system with specified retriever and reranker models.\n",
    "    \n",
    "    Args:\n",
    "        retriever_name (str): Name of the retriever model\n",
    "        reranker_name (str): Name of the reranker model\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (retriever, reranker) model instances\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    model_kwargs = {\"attn_implementation\": \"flash_attention_2\", 'dtype': torch.bfloat16}\n",
    "\n",
    "    retriever = SentenceTransformer(retriever_name, model_kwargs=model_kwargs)\n",
    "    reranker = CrossEncoder(reranker_name, model_kwargs=model_kwargs, trust_remote_code=True)\n",
    "\n",
    "    return retriever, reranker\n",
    "\n",
    "\n",
    "def run_experiment(corpus, question, name,\n",
    "                   retriever_name='ibm-granite/granite-embedding-english-r2',\n",
    "                   reranker_name='ibm-granite/granite-embedding-reranker-english-r2'):\n",
    "    \"\"\"\n",
    "    Run a search experiment with specified models and corpus.\n",
    "    \n",
    "    Args:\n",
    "        corpus (list): List of documents to search in\n",
    "        question (str): Query to search for\n",
    "        name (str): Name of the experiment\n",
    "        retriever_name (str): Name of the retriever model\n",
    "        reranker_name (str): Name of the reranker model\n",
    "    \"\"\"\n",
    "    print(f\"Running experiment {name}\")\n",
    "\n",
    "    # Create retriever and reranker\n",
    "    retriever, reranker = create_search_system(retriever_name, reranker_name)\n",
    "\n",
    "    # Step 1: Encode corpus once (can be cached)\n",
    "    corpus_embeddings = retriever.encode_document(corpus, convert_to_tensor=True)\n",
    "\n",
    "    # Step 2: Retrieve top-k candidates\n",
    "    def search(query, top_k=20):\n",
    "        query_embedding = retriever.encode_query(query, convert_to_tensor=True)\n",
    "\n",
    "        # Find top-k with retriever\n",
    "        hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=top_k)[0]\n",
    "\n",
    "        # Step 3: Rerank with cross-encoder\n",
    "        cross_inp = [(query, corpus[hit['corpus_id']]) for hit in hits]\n",
    "        cross_scores = reranker.predict(cross_inp)\n",
    "\n",
    "        # Sort by reranker scores\n",
    "        for idx, score in enumerate(cross_scores):\n",
    "            hits[idx]['rerank_score'] = score\n",
    "\n",
    "        hits = sorted(hits, key=lambda x: x['rerank_score'], reverse=True)\n",
    "\n",
    "        return hits[:5]  # Return top 5 after reranking\n",
    "\n",
    "    # Use it\n",
    "    results = search(question)\n",
    "    for hit in results:\n",
    "        print(f\"\\tScore: {hit['rerank_score']:.4f} | {corpus[hit['corpus_id']]}\")\n",
    "\n",
    "# Your document corpus\n",
    "corpus = [\n",
    "    \"Python is a high-level programming language\",\n",
    "    \"Machine learning models require training data\",\n",
    "    \"Machine learning is an engineeering discipline that studies best coding practices.\",\n",
    "    \"Natural language processing enables text understanding\",\n",
    "    \"Deep learning uses neural networks with multiple layers\",\n",
    "    \"Data science combines statistics and programming\",\n",
    "    \"Ana are mere.\"\n",
    "    # ... your documents here\n",
    "]\n",
    "corpus2 = [\n",
    "    \"Venus is often called Earth's twin because of its similar size and proximity.\",\n",
    "    \"Mars, known for its reddish appearance, is often referred to as the Red Planet.\",\n",
    "    \"Jupiter, the largest planet in our solar system, has a prominent red spot.\",\n",
    "    \"Saturn, famous for its rings, is sometimes mistaken for the Red Planet.\"\n",
    "]\n",
    "corpus3 = [\n",
    "    \"Romeo and Juliet is a play by William Shakespeare.\",\n",
    "    \"Climate change refers to long-term shifts in temperatures.\",\n",
    "    \"Shakespeare also wrote Hamlet and Macbeth.\",\n",
    "    \"Water is an inorganic compound with the chemical formula H2O.\",\n",
    "    \"In liquid form, H2O is also called 'water' at standard temperature and pressure.\"\n",
    "]"
   ],
   "id": "bf2c681a88b43854",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T17:55:59.901228Z",
     "start_time": "2025-10-08T17:55:56.233625Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# reranker = 'ibm-granite/granite-embedding-reranker-english-r2'\n",
    "reranker = '../models/149m_reranker_updated'\n",
    "run_experiment(corpus, \"What is machine learning?\", name=\"ML\", reranker_name=reranker)\n",
    "run_experiment(corpus2, \"What planet is known as the Red Planet?\", name=\"RedPlanet\", reranker_name=reranker)\n",
    "run_experiment(corpus3, \"what is the chemical formula of water?\", name=\"Water\", reranker_name=reranker)"
   ],
   "id": "5d494f5693aea63e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment ML\n",
      "\tScore: 1.0000 | Machine learning is an engineeering discipline that studies best coding practices.\n",
      "\tScore: 1.0000 | Machine learning models require training data\n",
      "\tScore: 1.0000 | Data science combines statistics and programming\n",
      "\tScore: 1.0000 | Deep learning uses neural networks with multiple layers\n",
      "\tScore: 1.0000 | Natural language processing enables text understanding\n",
      "Running experiment RedPlanet\n",
      "\tScore: 1.0000 | Mars, known for its reddish appearance, is often referred to as the Red Planet.\n",
      "\tScore: 1.0000 | Saturn, famous for its rings, is sometimes mistaken for the Red Planet.\n",
      "\tScore: 1.0000 | Jupiter, the largest planet in our solar system, has a prominent red spot.\n",
      "\tScore: 1.0000 | Venus is often called Earth's twin because of its similar size and proximity.\n",
      "Running experiment Water\n",
      "\tScore: 1.0000 | Water is an inorganic compound with the chemical formula H2O.\n",
      "\tScore: 1.0000 | In liquid form, H2O is also called 'water' at standard temperature and pressure.\n",
      "\tScore: 0.5859 | Climate change refers to long-term shifts in temperatures.\n",
      "\tScore: 0.0347 | Shakespeare also wrote Hamlet and Macbeth.\n",
      "\tScore: 0.0170 | Romeo and Juliet is a play by William Shakespeare.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T17:02:54.695275Z",
     "start_time": "2025-10-08T17:02:53.688168Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# reranker = 'ibm-granite/granite-embedding-reranker-english-r2'\n",
    "reranker = 'Alibaba-NLP/gte-multilingual-reranker-base'\n",
    "run_experiment(corpus, \"What is machine learning?\", name=\"ML\", reranker_name=reranker)\n",
    "run_experiment(corpus2, \"What planet is known as the Red Planet?\", name=\"RedPlanet\", reranker_name=reranker)"
   ],
   "id": "9988edc320a02bde",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment ML\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Alibaba-NLP/new-impl You can inspect the repository content at https://hf.co/Alibaba-NLP/gte-multilingual-reranker-base.\nPlease pass the argument `trust_remote_code=True` to allow custom code to be run.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[56]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# reranker = 'ibm-granite/granite-embedding-reranker-english-r2'\u001B[39;00m\n\u001B[32m      2\u001B[39m reranker = \u001B[33m'\u001B[39m\u001B[33mAlibaba-NLP/gte-multilingual-reranker-base\u001B[39m\u001B[33m'\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m \u001B[43mrun_experiment\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcorpus\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mWhat is machine learning?\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mML\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreranker_name\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreranker\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      4\u001B[39m run_experiment(corpus2, \u001B[33m\"\u001B[39m\u001B[33mWhat planet is known as the Red Planet?\u001B[39m\u001B[33m\"\u001B[39m, name=\u001B[33m\"\u001B[39m\u001B[33mRedPlanet\u001B[39m\u001B[33m\"\u001B[39m, reranker_name=reranker)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[53]\u001B[39m\u001B[32m, line 37\u001B[39m, in \u001B[36mrun_experiment\u001B[39m\u001B[34m(corpus, question, name, retriever_name, reranker_name)\u001B[39m\n\u001B[32m     34\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mRunning experiment \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m     36\u001B[39m \u001B[38;5;66;03m# Create retriever and reranker\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m37\u001B[39m retriever, reranker = \u001B[43mcreate_search_system\u001B[49m\u001B[43m(\u001B[49m\u001B[43mretriever_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreranker_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     39\u001B[39m \u001B[38;5;66;03m# Step 1: Encode corpus once (can be cached)\u001B[39;00m\n\u001B[32m     40\u001B[39m corpus_embeddings = retriever.encode_document(corpus, convert_to_tensor=\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[53]\u001B[39m\u001B[32m, line 17\u001B[39m, in \u001B[36mcreate_search_system\u001B[39m\u001B[34m(retriever_name, reranker_name)\u001B[39m\n\u001B[32m     14\u001B[39m model_kwargs = {\u001B[33m\"\u001B[39m\u001B[33mattn_implementation\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mflash_attention_2\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mdtype\u001B[39m\u001B[33m'\u001B[39m: torch.bfloat16}\n\u001B[32m     16\u001B[39m retriever = SentenceTransformer(retriever_name, model_kwargs=model_kwargs)\n\u001B[32m---> \u001B[39m\u001B[32m17\u001B[39m reranker = \u001B[43mCrossEncoder\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreranker_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcache_folder\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m     19\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m retriever, reranker\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniforge3/envs/docu/lib/python3.12/site-packages/sentence_transformers/cross_encoder/util.py:39\u001B[39m, in \u001B[36mcross_encoder_init_args_decorator.<locals>.wrapper\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m     36\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m     37\u001B[39m         kwargs[\u001B[33m\"\u001B[39m\u001B[33mconfig_kwargs\u001B[39m\u001B[33m\"\u001B[39m][\u001B[33m\"\u001B[39m\u001B[33mclassifier_dropout\u001B[39m\u001B[33m\"\u001B[39m] = classifier_dropout\n\u001B[32m---> \u001B[39m\u001B[32m39\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniforge3/envs/docu/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:149\u001B[39m, in \u001B[36mCrossEncoder.__init__\u001B[39m\u001B[34m(self, model_name_or_path, num_labels, max_length, activation_fn, device, cache_folder, trust_remote_code, revision, local_files_only, token, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data, backend)\u001B[39m\n\u001B[32m    146\u001B[39m \u001B[38;5;28mself\u001B[39m._model_card_text = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    147\u001B[39m \u001B[38;5;28mself\u001B[39m.backend = backend\n\u001B[32m--> \u001B[39m\u001B[32m149\u001B[39m config: PretrainedConfig = \u001B[43mAutoConfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    150\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodel_name_or_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    151\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcache_folder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    152\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtrust_remote_code\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtrust_remote_code\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    153\u001B[39m \u001B[43m    \u001B[49m\u001B[43mrevision\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    154\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    155\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    156\u001B[39m \u001B[43m    \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mconfig_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    157\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    158\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(config, \u001B[33m\"\u001B[39m\u001B[33msentence_transformers\u001B[39m\u001B[33m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mversion\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m config.sentence_transformers:\n\u001B[32m    159\u001B[39m     model_version = config.sentence_transformers[\u001B[33m\"\u001B[39m\u001B[33mversion\u001B[39m\u001B[33m\"\u001B[39m]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniforge3/envs/docu/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py:1297\u001B[39m, in \u001B[36mAutoConfig.from_pretrained\u001B[39m\u001B[34m(cls, pretrained_model_name_or_path, **kwargs)\u001B[39m\n\u001B[32m   1295\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1296\u001B[39m         upstream_repo = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1297\u001B[39m     trust_remote_code = \u001B[43mresolve_trust_remote_code\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1298\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtrust_remote_code\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhas_local_code\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhas_remote_code\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mupstream_repo\u001B[49m\n\u001B[32m   1299\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1301\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m has_remote_code \u001B[38;5;129;01mand\u001B[39;00m trust_remote_code:\n\u001B[32m   1302\u001B[39m     config_class = get_class_from_dynamic_module(\n\u001B[32m   1303\u001B[39m         class_ref, pretrained_model_name_or_path, code_revision=code_revision, **kwargs\n\u001B[32m   1304\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniforge3/envs/docu/lib/python3.12/site-packages/transformers/dynamic_module_utils.py:747\u001B[39m, in \u001B[36mresolve_trust_remote_code\u001B[39m\u001B[34m(trust_remote_code, model_name, has_local_code, has_remote_code, error_message, upstream_repo)\u001B[39m\n\u001B[32m    744\u001B[39m         _raise_timeout_error(\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[32m    746\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m has_remote_code \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m has_local_code \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m trust_remote_code:\n\u001B[32m--> \u001B[39m\u001B[32m747\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m    748\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00merror_message\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m You can inspect the repository content at https://hf.co/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m    749\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mPlease pass the argument `trust_remote_code=True` to allow custom code to be run.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    750\u001B[39m     )\n\u001B[32m    752\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m trust_remote_code\n",
      "\u001B[31mValueError\u001B[39m: Alibaba-NLP/new-impl You can inspect the repository content at https://hf.co/Alibaba-NLP/gte-multilingual-reranker-base.\nPlease pass the argument `trust_remote_code=True` to allow custom code to be run."
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load model with long context support\n",
    "model = SentenceTransformer('ibm-granite/granite-embedding-english-r2')"
   ],
   "id": "34174f0e01707f78"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T21:40:19.095192Z",
     "start_time": "2025-10-02T21:40:19.020504Z"
    }
   },
   "cell_type": "code",
   "source": [
    "long_document=\"\"\"\n",
    "IBM Research introduces next-generation embedding models that don’t compromise between speed and accuracy\n",
    "\n",
    "When it comes to enterprise information retrieval, organizations face a persistent challenge: existing embedding models force you to choose between accuracy and speed, between long-context support and commercial licensing, between general-purpose performance and domain-specific excellence.\n",
    "\n",
    "On August 15th , we’ve introduced the Granite Embedding R2 models — a comprehensive family of retrieval models designed to eliminate these tradeoffs.\n",
    "\n",
    "What’s New in R2?\n",
    "The Granite Embedding R2 release includes three models, all available under Apache 2.0 license:\n",
    "\n",
    "granite-embedding-english-r2 (149M parameters): Our flagship model with 768-dimensional embeddings\n",
    "granite-embedding-small-english-r2 (47M parameters): A first-of-its-kind efficient model with 384-dimensional embeddings\n",
    "granite-embedding-reranker-english-r2 (149M parameters): A cross-encoder for precision ranking\n",
    "These models deliver three critical improvements over our first-generation release:\n",
    "\n",
    "16x expanded context length from 512 to 8,192 tokens — meeting modern document processing requirements\n",
    "19–44% faster inference than comparable models, without sacrificing accuracy\n",
    "State-of-the-art performance across text, code, long-documents, conversational queries, and tabular data\n",
    "Getting Started: Basic Usage\n",
    "Using Granite Embedding models is straightforward with the Sentence-Transformers library:\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "# Load the model\n",
    "model = SentenceTransformer('ibm-granite/granite-embedding-english-r2')\n",
    "# Encode some text\n",
    "documents = [\n",
    "    \"Granite models are designed for enterprise applications\",\n",
    "    \"Information retrieval systems need fast and accurate embeddings\",\n",
    "    \"Machine learning models can process natural language\"\n",
    "]\n",
    "# Generate embeddings\n",
    "embeddings = model.encode(documents)\n",
    "print(f\"Embedding shape: {embeddings.shape}\")  # (3, 768)\n",
    "For semantic search, you can compute similarity scores:\n",
    "\n",
    "import numpy as np\n",
    "from sentence_transformers import util\n",
    "# Encode query and documents\n",
    "query = \"What are enterprise AI models?\"\n",
    "query_embedding = model.encode(query)\n",
    "doc_embeddings = model.encode(documents)\n",
    "# Compute cosine similarity\n",
    "similarities = util.cos_sim(query_embedding, doc_embeddings)\n",
    "print(f\"Similarities: {similarities}\")\n",
    "# Get most relevant document\n",
    "best_idx = np.argmax(similarities)\n",
    "print(f\"Most relevant: {documents[best_idx]}\")\n",
    "Building a Semantic Search System\n",
    "Here’s a complete example building a retrieval system with the reranker:\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "import numpy as np\n",
    "# Load retriever and reranker\n",
    "retriever = SentenceTransformer('ibm-granite/granite-embedding-english-r2')\n",
    "reranker = CrossEncoder('ibm-granite/granite-embedding-reranker-english-r2')\n",
    "# Your document corpus\n",
    "corpus = [\n",
    "    \"Python is a high-level programming language\",\n",
    "    \"Machine learning models require training data\",\n",
    "    \"Natural language processing enables text understanding\",\n",
    "    \"Deep learning uses neural networks with multiple layers\",\n",
    "    \"Data science combines statistics and programming\",\n",
    "    # ... your documents here\n",
    "]\n",
    "# Step 1: Encode corpus once (can be cached)\n",
    "corpus_embeddings = retriever.encode(corpus, convert_to_tensor=True)\n",
    "# Step 2: Retrieve top-k candidates\n",
    "def search(query, top_k=20):\n",
    "    query_embedding = retriever.encode(query, convert_to_tensor=True)\n",
    "\n",
    "    # Find top-k with retriever\n",
    "    hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=top_k)[0]\n",
    "\n",
    "    # Step 3: Rerank with cross-encoder\n",
    "    cross_inp = [[query, corpus[hit['corpus_id']]] for hit in hits]\n",
    "    cross_scores = reranker.predict(cross_inp)\n",
    "\n",
    "    # Sort by reranker scores\n",
    "    for idx, score in enumerate(cross_scores):\n",
    "        hits[idx]['rerank_score'] = score\n",
    "\n",
    "    hits = sorted(hits, key=lambda x: x['rerank_score'], reverse=True)\n",
    "\n",
    "    return hits[:5]  # Return top 5 after reranking\n",
    "# Use it\n",
    "results = search(\"What is machine learning?\")\n",
    "for hit in results:\n",
    "    print(f\"Score: {hit['rerank_score']:.4f} | {corpus[hit['corpus_id']]}\")\n",
    "Long Context Documents\n",
    "Granite R2 handles up to 8,192 tokens, perfect for processing full documents:\n",
    "\n",
    "# Load model with long context support\n",
    "model = SentenceTransformer('ibm-granite/granite-embedding-english-r2')\n",
    "# Process a long document (e.g., research paper, technical documentation)\n",
    "long_document =\n",
    "[Your 5000+ word document here]\n",
    "This could be an entire research paper, technical manual,\n",
    "or any long-form content...\n",
    "\n",
    "# Encode the full document (no chunking needed for <8192 tokens)\n",
    "doc_embedding = model.encode(long_document, show_progress_bar=True)\n",
    "# Compare with shorter query\n",
    "query = \"What are the main findings of this research?\"\n",
    "query_embedding = model.encode(query)\n",
    "similarity = util.cos_sim(query_embedding, doc_embedding)\n",
    "print(f\"Relevance score: {similarity.item():.4f}\")\n",
    "Code Search Example\n",
    "Granite R2 excels at code retrieval:\n",
    "\n",
    "# Code snippets corpus\n",
    "code_snippets = [\n",
    "\n",
    "    def binary_search(arr, target):\n",
    "        left, right = 0, len(arr) - 1\n",
    "        while left <= right:\n",
    "            mid = (left + right) // 2\n",
    "            if arr[mid] == target:\n",
    "                return mid\n",
    "            elif arr[mid] < target:\n",
    "                left = mid + 1\n",
    "            else:\n",
    "                right = mid - 1\n",
    "        return -1\n",
    "\n",
    "    def quicksort(arr):\n",
    "        if len(arr) <= 1:\n",
    "            return arr\n",
    "        pivot = arr[len(arr) // 2]\n",
    "        left = [x for x in arr if x < pivot]\n",
    "        middle = [x for x in arr if x == pivot]\n",
    "        right = [x for x in arr if x > pivot]\n",
    "        return quicksort(left) + middle + quicksort(right)\n",
    "    class LinkedList:\n",
    "        def __init__(self):\n",
    "            self.head = None\n",
    "\n",
    "        def append(self, data):\n",
    "            if not self.head:\n",
    "                self.head = Node(data)\n",
    "                return\n",
    "            current = self.head\n",
    "            while current.next:\n",
    "                current = current.next\n",
    "            current.next = Node(data)\n",
    "]\n",
    "# Encode code\n",
    "code_embeddings = model.encode(code_snippets)\n",
    "# Natural language query\n",
    "query = \"How do I implement a binary search algorithm?\"\n",
    "query_embedding = model.encode(query)\n",
    "# Find most relevant code\n",
    "similarities = util.cos_sim(query_embedding, code_embeddings)[0]\n",
    "best_match = np.argmax(similarities)\n",
    "print(f\"Most relevant code snippet:\\n{code_snippets[best_match]}\")\n",
    "Table Retrieval\n",
    "Handle structured data with ease:\n",
    "\n",
    "# Tables in markdown format\n",
    "tables = [\n",
    "    | Product | Q1 Revenue | Q2 Revenue |\n",
    "    |---------|-----------|-----------|\n",
    "    | Product A | $500K | $650K |\n",
    "    | Product B | $300K | $420K |\n",
    "    | Employee | Department | Salary |\n",
    "    |----------|-----------|--------|\n",
    "    | John Doe | Engineering | $120K |\n",
    "    | Jane Smith | Marketing | $95K |\n",
    "    | Country | Population | GDP |\n",
    "    |---------|-----------|-----|\n",
    "    | USA | 331M | $21T |\n",
    "    | China | 1.4B | $14T |\n",
    "\n",
    "]\n",
    "# Encode tables\n",
    "table_embeddings = model.encode(tables)\n",
    "# Query for specific information\n",
    "query = \"What was the revenue growth for our products?\"\n",
    "query_embedding = model.encode(query)\n",
    "similarities = util.cos_sim(query_embedding, table_embeddings)[0]\n",
    "best_table = np.argmax(similarities)\n",
    "print(f\"Most relevant table:\\n{tables[best_table]}\")\n",
    "Batch Processing for Production\n",
    "For production deployments processing large volumes:\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "# Load model with GPU support\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = SentenceTransformer('ibm-granite/granite-embedding-english-r2', device=device)\n",
    "# Large batch of documents\n",
    "documents = [...] # Your thousands of documents\n",
    "# Efficient batch encoding\n",
    "batch_size = 128\n",
    "all_embeddings = model.encode(\n",
    "    documents,\n",
    "    batch_size=batch_size,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_tensor=True,\n",
    "    normalize_embeddings=True  # For cosine similarity\n",
    ")\n",
    "# Save embeddings for later use\n",
    "torch.save(all_embeddings, 'document_embeddings.pt')\n",
    "# Load and search later\n",
    "embeddings = torch.load('document_embeddings.pt')\n",
    "query_emb = model.encode(query, convert_to_tensor=True)\n",
    "hits = util.semantic_search(query_emb, embeddings, top_k=10)\n",
    "Multi-Turn Conversational Search\n",
    "Handle conversational context:\n",
    "\n",
    "conversation_history = [\n",
    "    \"What are the system requirements for the new software?\",\n",
    "    \"Does it work on Mac?\",\n",
    "    \"What about Linux distributions?\"\n",
    "]\n",
    "# Concatenate conversation context\n",
    "context = \" \".join(conversation_history)\n",
    "current_query = conversation_history[-1]\n",
    "# Encode with full context\n",
    "context_embedding = model.encode(context)\n",
    "# Search in your knowledge base\n",
    "results = search_with_context(context_embedding, knowledge_base)\n",
    "Built on Modern Foundations\n",
    "The R2 models leverage the ModernBERT architecture, incorporating recent advances in encoder design:\n",
    "\n",
    "Alternating attention mechanisms for efficiency\n",
    "Rotary positional embeddings enabling flexible context lengths\n",
    "Flash Attention support for optimized inference\n",
    "We trained these models on 2 trillion tokens from high-quality sources including GneissWeb, Wikipedia, and Granite Code data. Every dataset underwent comprehensive governance review, with screening for personal information and profanity — because enterprise deployments demand transparency and responsible AI practices.\n",
    "\n",
    "A Novel Training Pipeline\n",
    "What sets Granite R2 apart is our five-stage training methodology:\n",
    "\n",
    "1. Retrieval-Oriented Pretraining: Using RetroMAE to train rich [CLS] representations without explicit contrastive objectives\n",
    "\n",
    "2. Tabular Pretraining: A breakthrough approach for handling structured data. Traditional embedding models struggle with tables containing numerical data and limited context. Our solution? We generated synthetic summaries for 8 million tables using Mistral-7B, then modified the RetroMAE objective to predict masked tokens over summaries rather than table content itself. This forces the encoder to align table structure with natural language descriptions.\n",
    "\n",
    "3. Contrastive Finetuning: Training on large-scale semi-supervised pairs with improved contrastive loss\n",
    "\n",
    "4. Contrastive Distillation: Rather than simply finetuning on hard negatives, we distill knowledge from a Mistral-7B teacher model trained on high-quality triples. This approach yields larger performance gains than traditional hard-negative training.\n",
    "\n",
    "5. Domain Adaptation: Specialized training for multi-turn conversational retrieval\n",
    "\n",
    "This pipeline enables a single model family to excel across remarkably diverse tasks.\n",
    "\n",
    "Performance That Speaks for Itself\n",
    "We evaluated Granite R2 across comprehensive benchmarks:\n",
    "\n",
    "General Retrieval (MTEB-v2)\n",
    "\n",
    "granite-english-r2: 56.4 average score\n",
    "granite-small-r2: 53.9 average score\n",
    "Code Retrieval (COIR)\n",
    "\n",
    "54.8 / 53.4 across text-to-code, code-to-text, and hybrid tasks\n",
    "Zero-shot evaluation (no COIR training data used)\n",
    "Long-Context (MLDR, LongEmbed)\n",
    "\n",
    "granite-english-r2: 41.6 MLDR, 67.8 LongEmbed\n",
    "granite-small-r2: 40.1 MLDR, 61.9 LongEmbed\n",
    "State-of-the-art on LongEmbed benchmark\n",
    "Table Retrieval\n",
    "\n",
    "78.5 / 75.5 across OpenWikiTables, NQTables, OTT-QA, MultiHierTT, and AIT-QA\n",
    "Multi-Turn Conversational (MT-RAG)\n",
    "\n",
    "granite-english-r2: 57.6 Recall@5\n",
    "Substantial improvement over first-generation models\n",
    "Speed Without Compromise\n",
    "Performance benchmarks often overlook a critical real-world constraint: encoding speed. When you’re ingesting millions of documents with frequent updates, speed directly impacts operational costs and user experience.\n",
    "\n",
    "We benchmarked encoding speed using 23,000 IBM technical documents (averaging 6,393 characters, ranging from 10 to 475,001 characters):\n",
    "\n",
    "granite-english-r2: 144 documents/second\n",
    "granite-small-r2: 199 documents/second\n",
    "These speeds represent 19–44% improvements over leading competitors, despite the R2 models having slightly more parameters than R1. The ModernBERT architecture’s optimizations — particularly Flash Attention — enable this efficiency gain.\n",
    "\n",
    "Complete Retrieval Ecosystem\n",
    "The reranker model completes the retrieval pipeline. Built on granite-embedding-english-r2, it uses a PListMLE loss objective for position-aware ranking:\n",
    "\n",
    "BEIR: 55.4 (vs. 53.1 for retriever alone)\n",
    "MLDR: 44.4 (vs. 41.6 for retriever alone)\n",
    "This retrieve-and-rerank framework maximizes both recall and precision without severe computational overhead.\n",
    "\n",
    "Enterprise-Ready from Day One\n",
    "Every Granite model prioritizes enterprise requirements:\n",
    "\n",
    "Data Governance: Comprehensive clearance process capturing content description, intended use, data classification, licensing information, usage restrictions, and personal information assessment\n",
    "\n",
    "Licensing: Apache 2.0 — no restrictions on commercial use, no proprietary training data limitations\n",
    "\n",
    "Transparency: Fully documented training data sources, architectural decisions, and evaluation methodology\n",
    "\n",
    "Why This Matters\n",
    "Information retrieval isn’t just about finding documents — it’s about enabling AI systems to access relevant knowledge efficiently. Whether you’re building RAG applications, semantic search engines, or recommendation systems, embedding quality and speed determine what’s possible.\n",
    "\n",
    "Granite R2 models don’t force you to choose between accuracy and speed, between long-context support and efficiency, between general-purpose capability and domain-specific performance. They deliver all of it.\n",
    "\n",
    "In an era where milliseconds matter and accuracy cannot be compromised, Granite R2 doesn’t just meet the standard — it sets it.\n",
    "\n",
    "Get Started\n",
    "All Granite Embedding R2 models are available now on Hugging Face under Apache 2.0 license:\n",
    "\n",
    "granite-embedding-english-r2\n",
    "granite-embedding-small-english-r2\n",
    "granite-embedding-reranker-english-r2\n",
    "For technical details, architecture description, and comprehensive benchmark results, see our research paper.\n",
    "\n",
    "The Granite Embedding R2 models represent collaborative work across IBM Research teams in multiple geographies. For questions or feedback, visit our GitHub repository.\n",
    "\"\"\"\n",
    "# Encode the full document (no chunking needed for <8192 tokens)\n",
    "doc_embedding = model.encode(long_document, show_progress_bar=True)\n",
    "# Compare with shorter query\n",
    "query = \"What are the main findings of this research?\"\n",
    "query_embedding = model.encode(query)\n",
    "similarity = util.cos_sim(query_embedding, doc_embedding)\n",
    "print(f\"Relevance score: {similarity.item():.4f}\")"
   ],
   "id": "4b3d9c7247e533cc",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 22.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevance score: 0.7477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Code Search Example",
   "id": "70401c76b2c4ab34"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T21:41:03.655344Z",
     "start_time": "2025-10-02T21:41:03.652918Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Code snippets corpus\n",
    "code_snippets = [\n",
    "    \"\"\"\n",
    "    def binary_search(arr, target):\n",
    "        left, right = 0, len(arr) - 1\n",
    "        while left <= right:\n",
    "            mid = (left + right) // 2\n",
    "            if arr[mid] == target:\n",
    "                return mid\n",
    "            elif arr[mid] < target:\n",
    "                left = mid + 1\n",
    "            else:\n",
    "                right = mid - 1\n",
    "        return -1\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    def quicksort(arr):\n",
    "        if len(arr) <= 1:\n",
    "            return arr\n",
    "        pivot = arr[len(arr) // 2]\n",
    "        left = [x for x in arr if x < pivot]\n",
    "        middle = [x for x in arr if x == pivot]\n",
    "        right = [x for x in arr if x > pivot]\n",
    "        return quicksort(left) + middle + quicksort(right)\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    class LinkedList:\n",
    "        def __init__(self):\n",
    "            self.head = None\n",
    "\n",
    "        def append(self, data):\n",
    "            if not self.head:\n",
    "                self.head = Node(data)\n",
    "                return\n",
    "            current = self.head\n",
    "            while current.next:\n",
    "                current = current.next\n",
    "            current.next = Node(data)\n",
    "    \"\"\"\n",
    "]"
   ],
   "id": "1e2d958c8c7d1353",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T21:42:57.281638Z",
     "start_time": "2025-10-02T21:42:57.238468Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Encode code\n",
    "code_embeddings = model.encode(code_snippets)\n",
    "# Natural language query\n",
    "query = \"How do I implement a binary search algorithm?\"\n",
    "query_embedding = model.encode(query)\n",
    "# Find most relevant code\n",
    "similarities = util.cos_sim(query_embedding, code_embeddings)[0]\n",
    "best_match = np.argmax(similarities)\n",
    "print(f\"Most relevant code snippet:\\n{code_snippets[best_match]}\")\n"
   ],
   "id": "1aac12e98f98fd49",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most relevant code snippet:\n",
      "\n",
      "    class LinkedList:\n",
      "        def __init__(self):\n",
      "            self.head = None\n",
      "\n",
      "        def append(self, data):\n",
      "            if not self.head:\n",
      "                self.head = Node(data)\n",
      "                return\n",
      "            current = self.head\n",
      "            while current.next:\n",
      "                current = current.next\n",
      "            current.next = Node(data)\n",
      "    \n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T21:43:10.456758Z",
     "start_time": "2025-10-02T21:43:10.453452Z"
    }
   },
   "cell_type": "markdown",
   "source": "Table Retrieval",
   "id": "bf6f91051e61c4ce"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T21:43:30.842530Z",
     "start_time": "2025-10-02T21:43:30.840175Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Tables in markdown format\n",
    "tables = [\n",
    "    \"\"\"\n",
    "    | Product | Q1 Revenue | Q2 Revenue |\n",
    "    |---------|-----------|-----------|\n",
    "    | Product A | $500K | $650K |\n",
    "    | Product B | $300K | $420K |\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    | Employee | Department | Salary |\n",
    "    |----------|-----------|--------|\n",
    "    | John Doe | Engineering | $120K |\n",
    "    | Jane Smith | Marketing | $95K |\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    | Country | Population | GDP |\n",
    "    |---------|-----------|-----|\n",
    "    | USA | 331M | $21T |\n",
    "    | China | 1.4B | $14T |\n",
    "    \"\"\"\n",
    "]"
   ],
   "id": "42f471d78807a1f6",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T21:44:04.856174Z",
     "start_time": "2025-10-02T21:44:04.809768Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Encode tables\n",
    "table_embeddings = model.encode(tables)\n",
    "# Query for specific information\n",
    "query = \"What was the revenue growth for our products?\"\n",
    "query_embedding = model.encode(query)\n",
    "similarities = util.cos_sim(query_embedding, table_embeddings)[0]\n",
    "best_table = np.argmax(similarities)\n",
    "print(f\"Most relevant table:\\n{tables[best_table]}\")"
   ],
   "id": "5c89e8b352a61bc7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most relevant table:\n",
      "\n",
      "    | Product | Q1 Revenue | Q2 Revenue |\n",
      "    |---------|-----------|-----------|\n",
      "    | Product A | $500K | $650K |\n",
      "    | Product B | $300K | $420K |\n",
      "    \n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T21:44:12.194336Z",
     "start_time": "2025-10-02T21:44:12.191920Z"
    }
   },
   "cell_type": "markdown",
   "source": "Batch Processing for Production",
   "id": "efa52385ce012d59"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T21:44:24.564430Z",
     "start_time": "2025-10-02T21:44:24.562222Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch"
   ],
   "id": "97276dfd4353655d",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T21:44:50.125800Z",
     "start_time": "2025-10-02T21:44:48.633184Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load model with GPU support\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = SentenceTransformer('ibm-granite/granite-embedding-english-r2', device=device,\n",
    "                            model_kwargs=model_kwargs)\n",
    "# Large batch of documents\n",
    "# documents = [...] # Your thousands of documents\n",
    "documents = generate_text(1024, num_docs=128)\n",
    "# Efficient batch encoding\n",
    "batch_size = 64\n",
    "all_embeddings = model.encode_document(\n",
    "    documents,\n",
    "    batch_size=batch_size,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_tensor=True,\n",
    "    normalize_embeddings=True  # For cosine similarity\n",
    ")\n",
    "# Save embeddings for later use\n",
    "torch.save(all_embeddings, 'document_embeddings.pt')\n",
    "# Load and search later\n",
    "embeddings = torch.load('document_embeddings.pt')\n",
    "query_emb = model.encode(query, convert_to_tensor=True)\n",
    "hits = util.semantic_search(query_emb, embeddings, top_k=10)"
   ],
   "id": "98d40bbf0f019dfe",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches:   0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'ellipsis' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mTypeError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniforge3/envs/docu/lib/python3.12/site-packages/sentence_transformers/SentenceTransformer.py:1621\u001B[39m, in \u001B[36mSentenceTransformer.tokenize\u001B[39m\u001B[34m(self, texts, **kwargs)\u001B[39m\n\u001B[32m   1620\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1621\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m[\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtokenize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtexts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1622\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniforge3/envs/docu/lib/python3.12/site-packages/sentence_transformers/models/Transformer.py:307\u001B[39m, in \u001B[36mTransformer.tokenize\u001B[39m\u001B[34m(self, texts, padding)\u001B[39m\n\u001B[32m    306\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m text_tuple \u001B[38;5;129;01min\u001B[39;00m texts:\n\u001B[32m--> \u001B[39m\u001B[32m307\u001B[39m     batch1.append(\u001B[43mtext_tuple\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m]\u001B[49m)\n\u001B[32m    308\u001B[39m     batch2.append(text_tuple[\u001B[32m1\u001B[39m])\n",
      "\u001B[31mTypeError\u001B[39m: 'ellipsis' object is not subscriptable",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[31mTypeError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[44]\u001B[39m\u001B[32m, line 9\u001B[39m\n\u001B[32m      7\u001B[39m \u001B[38;5;66;03m# Efficient batch encoding\u001B[39;00m\n\u001B[32m      8\u001B[39m batch_size = \u001B[32m128\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m9\u001B[39m all_embeddings = \u001B[43mmodel\u001B[49m\u001B[43m.\u001B[49m\u001B[43mencode\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     10\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdocuments\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     11\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     12\u001B[39m \u001B[43m    \u001B[49m\u001B[43mshow_progress_bar\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m     13\u001B[39m \u001B[43m    \u001B[49m\u001B[43mconvert_to_tensor\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m     14\u001B[39m \u001B[43m    \u001B[49m\u001B[43mnormalize_embeddings\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# For cosine similarity\u001B[39;49;00m\n\u001B[32m     15\u001B[39m \u001B[43m)\u001B[49m\n\u001B[32m     16\u001B[39m \u001B[38;5;66;03m# Save embeddings for later use\u001B[39;00m\n\u001B[32m     17\u001B[39m torch.save(all_embeddings, \u001B[33m'\u001B[39m\u001B[33mdocument_embeddings.pt\u001B[39m\u001B[33m'\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniforge3/envs/docu/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001B[39m, in \u001B[36mcontext_decorator.<locals>.decorate_context\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    117\u001B[39m \u001B[38;5;129m@functools\u001B[39m.wraps(func)\n\u001B[32m    118\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mdecorate_context\u001B[39m(*args, **kwargs):\n\u001B[32m    119\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[32m--> \u001B[39m\u001B[32m120\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniforge3/envs/docu/lib/python3.12/site-packages/sentence_transformers/SentenceTransformer.py:1062\u001B[39m, in \u001B[36mSentenceTransformer.encode\u001B[39m\u001B[34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, truncate_dim, pool, chunk_size, **kwargs)\u001B[39m\n\u001B[32m   1060\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m start_index \u001B[38;5;129;01min\u001B[39;00m trange(\u001B[32m0\u001B[39m, \u001B[38;5;28mlen\u001B[39m(sentences), batch_size, desc=\u001B[33m\"\u001B[39m\u001B[33mBatches\u001B[39m\u001B[33m\"\u001B[39m, disable=\u001B[38;5;129;01mnot\u001B[39;00m show_progress_bar):\n\u001B[32m   1061\u001B[39m     sentences_batch = sentences_sorted[start_index : start_index + batch_size]\n\u001B[32m-> \u001B[39m\u001B[32m1062\u001B[39m     features = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtokenize\u001B[49m\u001B[43m(\u001B[49m\u001B[43msentences_batch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1063\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.device.type == \u001B[33m\"\u001B[39m\u001B[33mhpu\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m   1064\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33minput_ids\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m features:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniforge3/envs/docu/lib/python3.12/site-packages/sentence_transformers/SentenceTransformer.py:1623\u001B[39m, in \u001B[36mSentenceTransformer.tokenize\u001B[39m\u001B[34m(self, texts, **kwargs)\u001B[39m\n\u001B[32m   1621\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m[\u001B[32m0\u001B[39m].tokenize(texts, **kwargs)\n\u001B[32m   1622\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1623\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m[\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtokenize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtexts\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniforge3/envs/docu/lib/python3.12/site-packages/sentence_transformers/models/Transformer.py:307\u001B[39m, in \u001B[36mTransformer.tokenize\u001B[39m\u001B[34m(self, texts, padding)\u001B[39m\n\u001B[32m    305\u001B[39m batch1, batch2 = [], []\n\u001B[32m    306\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m text_tuple \u001B[38;5;129;01min\u001B[39;00m texts:\n\u001B[32m--> \u001B[39m\u001B[32m307\u001B[39m     batch1.append(\u001B[43mtext_tuple\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m]\u001B[49m)\n\u001B[32m    308\u001B[39m     batch2.append(text_tuple[\u001B[32m1\u001B[39m])\n\u001B[32m    309\u001B[39m to_tokenize = [batch1, batch2]\n",
      "\u001B[31mTypeError\u001B[39m: 'ellipsis' object is not subscriptable"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5759c373c033b1a6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
