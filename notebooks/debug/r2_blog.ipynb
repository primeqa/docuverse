{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-09T15:22:46.083840Z",
     "start_time": "2025-10-09T15:22:42.551493Z"
    }
   },
   "source": [
    "from numpy import ndarray\n",
    "# Load necessary packages\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder, util\n",
    "import numpy as np\n",
    "from sentence_transformers import util\n",
    "from torch import Tensor"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-09T19:40:13.194099Z",
     "start_time": "2025-10-09T19:40:09.650070Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load retriever and reranker\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model_kwargs = {\"dtype\": torch.bfloat16, 'attn_implementation': 'flash_attention_2'}\n",
    "else:\n",
    "    model_kwargs = {}\n",
    "\n",
    "granite_retriever = SentenceTransformer('ibm-granite/granite-embedding-english-r2',\n",
    "                                        model_kwargs=model_kwargs)\n",
    "granite_reranker = CrossEncoder('ibm-granite/granite-embedding-reranker-english-r2',\n",
    "                                model_kwargs=model_kwargs)\n",
    "gemma_retriever = SentenceTransformer(\"google/embeddinggemma-300m\",\n",
    "                                      model_kwargs=model_kwargs)"
   ],
   "id": "590e8a62aafe6965",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Encode some text\n",
    "documents = [\n",
    "    \"Granite models are designed for enterprise applications\",\n",
    "    \"Information retrieval systems need fast and accurate embeddings\",\n",
    "    \"Machine learning models can process natural language\"\n",
    "]"
   ],
   "id": "e5b984f5a419c795"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Encode query and documents\n",
    "model = granite_retriever\n",
    "query = \"What's the purpose of the granite models?\"\n",
    "query_embedding = model.encode(query)\n",
    "doc_embeddings = model.encode(documents)\n",
    "# Compute cosine similarity\n",
    "similarities = util.cos_sim(query_embedding, doc_embeddings)\n",
    "print(f\"Similarities: {similarities}\")\n",
    "# Get most relevant document\n",
    "best_idx = np.argmax(similarities)\n",
    "print(f\"Most relevant: {documents[best_idx]}\")"
   ],
   "id": "f28dbfcf3a6a1e0b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Building a Semantic Search System\n",
   "id": "f0183da1dc3fc257"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "import numpy as np"
   ],
   "id": "42503bd4af587878"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Your document corpus\n",
    "corpus = [\n",
    "    \"Python is a high-level programming language\",\n",
    "    \"Machine learning models require training data\",\n",
    "    \"Natural language processing enables text understanding\",\n",
    "    \"Deep learning uses neural networks with multiple layers\",\n",
    "    \"Data science combines statistics and programming\",\n",
    "    # ... your documents here\n",
    "]"
   ],
   "id": "53a4e0903c75b3b6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Step 1: Encode corpus once (can be cached)\n",
    "corpus_embeddings = granite_retriever.encode(corpus, convert_to_tensor=True)"
   ],
   "id": "73baa13c7f7bf15a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "corpus_embeddings[0]",
   "id": "652f44762226cf93"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-09T19:40:43.082812Z",
     "start_time": "2025-10-09T19:40:43.079020Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 2: Retrieve top-k candidates\n",
    "def search(corpus_embeddings, query, top_k=20,\n",
    "           retriever: SentenceTransformer = granite_retriever,\n",
    "           reranker: CrossEncoder|None = granite_reranker):\n",
    "    query_embedding = retriever.encode_query(query, convert_to_tensor=True)\n",
    "\n",
    "    # Find top-k with retriever\n",
    "    hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=top_k)[0]\n",
    "\n",
    "    # Step 3: Rerank with cross-encoder\n",
    "    if reranker is not None:\n",
    "        cross_inp = [[query, corpus[hit['corpus_id']]] for hit in hits]\n",
    "        cross_scores = reranker.predict(cross_inp)\n",
    "\n",
    "        # Sort by reranker scores\n",
    "        for idx, score in enumerate(cross_scores):\n",
    "            hits[idx]['rerank_score'] = score\n",
    "\n",
    "        hits = sorted(hits, key=lambda x: x['rerank_score'], reverse=True)\n",
    "\n",
    "    return hits[:5]  # Return top 5 after reranking"
   ],
   "id": "9ec61cc508580ab8",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Use it\n",
    "results = search(corpus_embeddings, \"What is machine learning?\")\n",
    "for hit in results:\n",
    "    print(f\"Score: {hit['rerank_score']:.4f} | {corpus[hit['corpus_id']]}\")"
   ],
   "id": "b45be282210bcf4b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# The granite embedding r2 blog is the input\n",
    "long_document=\"\"\"\n",
    "IBM Research introduces next-generation embedding models that don’t compromise between speed and accuracy\n",
    "\n",
    "When it comes to enterprise information retrieval, organizations face a persistent challenge: existing embedding models force you to choose between accuracy and speed, between long-context support and commercial licensing, between general-purpose performance and domain-specific excellence.\n",
    "\n",
    "On August 15th , we’ve introduced the Granite Embedding R2 models — a comprehensive family of retrieval models designed to eliminate these tradeoffs.\n",
    "\n",
    "What’s New in R2?\n",
    "The Granite Embedding R2 release includes three models, all available under Apache 2.0 license:\n",
    "\n",
    "granite-embedding-english-r2 (149M parameters): Our flagship model with 768-dimensional embeddings\n",
    "granite-embedding-small-english-r2 (47M parameters): A first-of-its-kind efficient model with 384-dimensional embeddings\n",
    "granite-embedding-reranker-english-r2 (149M parameters): A cross-encoder for precision ranking\n",
    "These models deliver three critical improvements over our first-generation release:\n",
    "\n",
    "16x expanded context length from 512 to 8,192 tokens — meeting modern document processing requirements\n",
    "19–44% faster inference than comparable models, without sacrificing accuracy\n",
    "State-of-the-art performance across text, code, long-documents, conversational queries, and tabular data\n",
    "Getting Started: Basic Usage\n",
    "Using Granite Embedding models is straightforward with the Sentence-Transformers library:\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "# Load the model\n",
    "model = SentenceTransformer('ibm-granite/granite-embedding-english-r2')\n",
    "# Encode some text\n",
    "documents = [\n",
    "    \"Granite models are designed for enterprise applications\",\n",
    "    \"Information retrieval systems need fast and accurate embeddings\",\n",
    "    \"Machine learning models can process natural language\"\n",
    "]\n",
    "# Generate embeddings\n",
    "embeddings = model.encode(documents)\n",
    "print(f\"Embedding shape: {embeddings.shape}\")  # (3, 768)\n",
    "For semantic search, you can compute similarity scores:\n",
    "\n",
    "import numpy as np\n",
    "from sentence_transformers import util\n",
    "# Encode query and documents\n",
    "query = \"What are enterprise AI models?\"\n",
    "query_embedding = model.encode(query)\n",
    "doc_embeddings = model.encode(documents)\n",
    "# Compute cosine similarity\n",
    "similarities = util.cos_sim(query_embedding, doc_embeddings)\n",
    "print(f\"Similarities: {similarities}\")\n",
    "# Get most relevant document\n",
    "best_idx = np.argmax(similarities)\n",
    "print(f\"Most relevant: {documents[best_idx]}\")\n",
    "Building a Semantic Search System\n",
    "Here’s a complete example building a retrieval system with the reranker:\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "import numpy as np\n",
    "# Load retriever and reranker\n",
    "retriever = SentenceTransformer('ibm-granite/granite-embedding-english-r2')\n",
    "reranker = CrossEncoder('ibm-granite/granite-embedding-reranker-english-r2')\n",
    "# Your document corpus\n",
    "corpus = [\n",
    "    \"Python is a high-level programming language\",\n",
    "    \"Machine learning models require training data\",\n",
    "    \"Natural language processing enables text understanding\",\n",
    "    \"Deep learning uses neural networks with multiple layers\",\n",
    "    \"Data science combines statistics and programming\",\n",
    "    # ... your documents here\n",
    "]\n",
    "# Step 1: Encode corpus once (can be cached)\n",
    "corpus_embeddings = retriever.encode(corpus, convert_to_tensor=True)\n",
    "# Step 2: Retrieve top-k candidates\n",
    "def search(query, top_k=20):\n",
    "    query_embedding = retriever.encode(query, convert_to_tensor=True)\n",
    "\n",
    "    # Find top-k with retriever\n",
    "    hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=top_k)[0]\n",
    "\n",
    "    # Step 3: Rerank with cross-encoder\n",
    "    cross_inp = [[query, corpus[hit['corpus_id']]] for hit in hits]\n",
    "    cross_scores = reranker.predict(cross_inp)\n",
    "\n",
    "    # Sort by reranker scores\n",
    "    for idx, score in enumerate(cross_scores):\n",
    "        hits[idx]['rerank_score'] = score\n",
    "\n",
    "    hits = sorted(hits, key=lambda x: x['rerank_score'], reverse=True)\n",
    "\n",
    "    return hits[:5]  # Return top 5 after reranking\n",
    "# Use it\n",
    "results = search(\"What is machine learning?\")\n",
    "for hit in results:\n",
    "    print(f\"Score: {hit['rerank_score']:.4f} | {corpus[hit['corpus_id']]}\")\n",
    "Long Context Documents\n",
    "Granite R2 handles up to 8,192 tokens, perfect for processing full documents:\n",
    "\n",
    "# Load model with long context support\n",
    "model = SentenceTransformer('ibm-granite/granite-embedding-english-r2')\n",
    "# Process a long document (e.g., research paper, technical documentation)\n",
    "long_document =\n",
    "[Your 5000+ word document here]\n",
    "This could be an entire research paper, technical manual,\n",
    "or any long-form content...\n",
    "\n",
    "# Encode the full document (no chunking needed for <8192 tokens)\n",
    "doc_embedding = model.encode(long_document, show_progress_bar=True)\n",
    "# Compare with shorter query\n",
    "query = \"What are the main findings of this research?\"\n",
    "query_embedding = model.encode(query)\n",
    "similarity = util.cos_sim(query_embedding, doc_embedding)\n",
    "print(f\"Relevance score: {similarity.item():.4f}\")\n",
    "Code Search Example\n",
    "Granite R2 excels at code retrieval:\n",
    "\n",
    "# Code snippets corpus\n",
    "code_snippets = [\n",
    "\n",
    "    def binary_search(arr, target):\n",
    "        left, right = 0, len(arr) - 1\n",
    "        while left <= right:\n",
    "            mid = (left + right) // 2\n",
    "            if arr[mid] == target:\n",
    "                return mid\n",
    "            elif arr[mid] < target:\n",
    "                left = mid + 1\n",
    "            else:\n",
    "                right = mid - 1\n",
    "        return -1\n",
    "\n",
    "    def quicksort(arr):\n",
    "        if len(arr) <= 1:\n",
    "            return arr\n",
    "        pivot = arr[len(arr) // 2]\n",
    "        left = [x for x in arr if x < pivot]\n",
    "        middle = [x for x in arr if x == pivot]\n",
    "        right = [x for x in arr if x > pivot]\n",
    "        return quicksort(left) + middle + quicksort(right)\n",
    "    class LinkedList:\n",
    "        def __init__(self):\n",
    "            self.head = None\n",
    "\n",
    "        def append(self, data):\n",
    "            if not self.head:\n",
    "                self.head = Node(data)\n",
    "                return\n",
    "            current = self.head\n",
    "            while current.next:\n",
    "                current = current.next\n",
    "            current.next = Node(data)\n",
    "]\n",
    "# Encode code\n",
    "code_embeddings = model.encode(code_snippets)\n",
    "# Natural language query\n",
    "query = \"How do I implement a binary search algorithm?\"\n",
    "query_embedding = model.encode(query)\n",
    "# Find most relevant code\n",
    "similarities = util.cos_sim(query_embedding, code_embeddings)[0]\n",
    "best_match = np.argmax(similarities)\n",
    "print(f\"Most relevant code snippet:\\n{code_snippets[best_match]}\")\n",
    "Table Retrieval\n",
    "Handle structured data with ease:\n",
    "\n",
    "# Tables in markdown format\n",
    "tables = [\n",
    "    | Product | Q1 Revenue | Q2 Revenue |\n",
    "    |---------|-----------|-----------|\n",
    "    | Product A | $500K | $650K |\n",
    "    | Product B | $300K | $420K |\n",
    "    | Employee | Department | Salary |\n",
    "    |----------|-----------|--------|\n",
    "    | John Doe | Engineering | $120K |\n",
    "    | Jane Smith | Marketing | $95K |\n",
    "    | Country | Population | GDP |\n",
    "    |---------|-----------|-----|\n",
    "    | USA | 331M | $21T |\n",
    "    | China | 1.4B | $14T |\n",
    "\n",
    "]\n",
    "# Encode tables\n",
    "table_embeddings = model.encode(tables)\n",
    "# Query for specific information\n",
    "query = \"What was the revenue growth for our products?\"\n",
    "query_embedding = model.encode(query)\n",
    "similarities = util.cos_sim(query_embedding, table_embeddings)[0]\n",
    "best_table = np.argmax(similarities)\n",
    "print(f\"Most relevant table:\\n{tables[best_table]}\")\n",
    "Batch Processing for Production\n",
    "For production deployments processing large volumes:\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "# Load model with GPU support\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = SentenceTransformer('ibm-granite/granite-embedding-english-r2', device=device)\n",
    "# Large batch of documents\n",
    "documents = [...] # Your thousands of documents\n",
    "# Efficient batch encoding\n",
    "batch_size = 128\n",
    "all_embeddings = model.encode(\n",
    "    documents,\n",
    "    batch_size=batch_size,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_tensor=True,\n",
    "    normalize_embeddings=True  # For cosine similarity\n",
    ")\n",
    "# Save embeddings for later use\n",
    "torch.save(all_embeddings, 'document_embeddings.pt')\n",
    "# Load and search later\n",
    "embeddings = torch.load('document_embeddings.pt')\n",
    "query_emb = model.encode(query, convert_to_tensor=True)\n",
    "hits = util.semantic_search(query_emb, embeddings, top_k=10)\n",
    "Multi-Turn Conversational Search\n",
    "Handle conversational context:\n",
    "\n",
    "conversation_history = [\n",
    "    \"What are the system requirements for the new software?\",\n",
    "    \"Does it work on Mac?\",\n",
    "    \"What about Linux distributions?\"\n",
    "]\n",
    "# Concatenate conversation context\n",
    "context = \" \".join(conversation_history)\n",
    "current_query = conversation_history[-1]\n",
    "# Encode with full context\n",
    "context_embedding = model.encode(context)\n",
    "# Search in your knowledge base\n",
    "results = search_with_context(context_embedding, knowledge_base)\n",
    "Built on Modern Foundations\n",
    "The R2 models leverage the ModernBERT architecture, incorporating recent advances in encoder design:\n",
    "\n",
    "Alternating attention mechanisms for efficiency\n",
    "Rotary positional embeddings enabling flexible context lengths\n",
    "Flash Attention support for optimized inference\n",
    "We trained these models on 2 trillion tokens from high-quality sources including GneissWeb, Wikipedia, and Granite Code data. Every dataset underwent comprehensive governance review, with screening for personal information and profanity — because enterprise deployments demand transparency and responsible AI practices.\n",
    "\n",
    "A Novel Training Pipeline\n",
    "What sets Granite R2 apart is our five-stage training methodology:\n",
    "\n",
    "1. Retrieval-Oriented Pretraining: Using RetroMAE to train rich [CLS] representations without explicit contrastive objectives\n",
    "\n",
    "2. Tabular Pretraining: A breakthrough approach for handling structured data. Traditional embedding models struggle with tables containing numerical data and limited context. Our solution? We generated synthetic summaries for 8 million tables using Mistral-7B, then modified the RetroMAE objective to predict masked tokens over summaries rather than table content itself. This forces the encoder to align table structure with natural language descriptions.\n",
    "\n",
    "3. Contrastive Finetuning: Training on large-scale semi-supervised pairs with improved contrastive loss\n",
    "\n",
    "4. Contrastive Distillation: Rather than simply finetuning on hard negatives, we distill knowledge from a Mistral-7B teacher model trained on high-quality triples. This approach yields larger performance gains than traditional hard-negative training.\n",
    "\n",
    "5. Domain Adaptation: Specialized training for multi-turn conversational retrieval\n",
    "\n",
    "This pipeline enables a single model family to excel across remarkably diverse tasks.\n",
    "\n",
    "Performance That Speaks for Itself\n",
    "We evaluated Granite R2 across comprehensive benchmarks:\n",
    "\n",
    "General Retrieval (MTEB-v2)\n",
    "\n",
    "granite-english-r2: 56.4 average score\n",
    "granite-small-r2: 53.9 average score\n",
    "Code Retrieval (COIR)\n",
    "\n",
    "54.8 / 53.4 across text-to-code, code-to-text, and hybrid tasks\n",
    "Zero-shot evaluation (no COIR training data used)\n",
    "Long-Context (MLDR, LongEmbed)\n",
    "\n",
    "granite-english-r2: 41.6 MLDR, 67.8 LongEmbed\n",
    "granite-small-r2: 40.1 MLDR, 61.9 LongEmbed\n",
    "State-of-the-art on LongEmbed benchmark\n",
    "Table Retrieval\n",
    "\n",
    "78.5 / 75.5 across OpenWikiTables, NQTables, OTT-QA, MultiHierTT, and AIT-QA\n",
    "Multi-Turn Conversational (MT-RAG)\n",
    "\n",
    "granite-english-r2: 57.6 Recall@5\n",
    "Substantial improvement over first-generation models\n",
    "Speed Without Compromise\n",
    "Performance benchmarks often overlook a critical real-world constraint: encoding speed. When you’re ingesting millions of documents with frequent updates, speed directly impacts operational costs and user experience.\n",
    "\n",
    "We benchmarked encoding speed using 23,000 IBM technical documents (averaging 6,393 characters, ranging from 10 to 475,001 characters):\n",
    "\n",
    "granite-english-r2: 144 documents/second\n",
    "granite-small-r2: 199 documents/second\n",
    "These speeds represent 19–44% improvements over leading competitors, despite the R2 models having slightly more parameters than R1. The ModernBERT architecture’s optimizations — particularly Flash Attention — enable this efficiency gain.\n",
    "\n",
    "Complete Retrieval Ecosystem\n",
    "The reranker model completes the retrieval pipeline. Built on granite-embedding-english-r2, it uses a PListMLE loss objective for position-aware ranking:\n",
    "\n",
    "BEIR: 55.4 (vs. 53.1 for retriever alone)\n",
    "MLDR: 44.4 (vs. 41.6 for retriever alone)\n",
    "This retrieve-and-rerank framework maximizes both recall and precision without severe computational overhead.\n",
    "\n",
    "Enterprise-Ready from Day One\n",
    "Every Granite model prioritizes enterprise requirements:\n",
    "\n",
    "Data Governance: Comprehensive clearance process capturing content description, intended use, data classification, licensing information, usage restrictions, and personal information assessment\n",
    "\n",
    "Licensing: Apache 2.0 — no restrictions on commercial use, no proprietary training data limitations\n",
    "\n",
    "Transparency: Fully documented training data sources, architectural decisions, and evaluation methodology\n",
    "\n",
    "Why This Matters\n",
    "Information retrieval isn’t just about finding documents — it’s about enabling AI systems to access relevant knowledge efficiently. Whether you’re building RAG applications, semantic search engines, or recommendation systems, embedding quality and speed determine what’s possible.\n",
    "\n",
    "Granite R2 models don’t force you to choose between accuracy and speed, between long-context support and efficiency, between general-purpose capability and domain-specific performance. They deliver all of it.\n",
    "\n",
    "In an era where milliseconds matter and accuracy cannot be compromised, Granite R2 doesn’t just meet the standard — it sets it.\n",
    "\n",
    "Get Started\n",
    "All Granite Embedding R2 models are available now on Hugging Face under Apache 2.0 license:\n",
    "\n",
    "granite-embedding-english-r2\n",
    "granite-embedding-small-english-r2\n",
    "granite-embedding-reranker-english-r2\n",
    "For technical details, architecture description, and comprehensive benchmark results, see our research paper.\n",
    "\n",
    "The Granite Embedding R2 models represent collaborative work across IBM Research teams in multiple geographies. For questions or feedback, visit our GitHub repository.\n",
    "\"\"\"\n",
    "# Encode the full document (no chunking needed for <8192 tokens)\n",
    "doc_embedding = model.encode(long_document, show_progress_bar=True)\n",
    "# Compare with shorter query\n",
    "query = \"What are the good blogs on granite R2 embeddings?\"\n",
    "query_embedding = model.encode(query)\n",
    "similarity = util.cos_sim(query_embedding, doc_embedding)\n",
    "print(f\"Relevance score for '{query}: {similarity.item():.4f}\")\n",
    "query = \"How do you lose weight fast?\"\n",
    "query_embedding = model.encode(query)\n",
    "similarity = util.cos_sim(query_embedding, doc_embedding)\n",
    "print(f\"Relevance score for '{query}': {similarity.item():.4f}\")"
   ],
   "id": "188b190e358b2943"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Code Search Example",
   "id": "c3133ca9019c45c3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Code snippets corpus\n",
    "code_snippets = [\n",
    "    \"\"\"\n",
    "    def binary_search(arr, target):\n",
    "        left, right = 0, len(arr) - 1\n",
    "        while left <= right:\n",
    "            mid = (left + right) // 2\n",
    "            if arr[mid] == target:\n",
    "                return mid\n",
    "            elif arr[mid] < target:\n",
    "                left = mid + 1\n",
    "            else:\n",
    "                right = mid - 1\n",
    "        return -1\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    def quicksort(arr):\n",
    "        if len(arr) <= 1:\n",
    "            return arr\n",
    "        pivot = arr[len(arr) // 2]\n",
    "        left = [x for x in arr if x < pivot]\n",
    "        middle = [x for x in arr if x == pivot]\n",
    "        right = [x for x in arr if x > pivot]\n",
    "        return quicksort(left) + middle + quicksort(right)\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    class LinkedList:\n",
    "        def __init__(self):\n",
    "            self.head = None\n",
    "\n",
    "        def append(self, data):\n",
    "            if not self.head:\n",
    "                self.head = Node(data)\n",
    "                return\n",
    "            current = self.head\n",
    "            while current.next:\n",
    "                current = current.next\n",
    "            current.next = Node(data)\n",
    "    \"\"\"\n",
    "]"
   ],
   "id": "a751b0ea94122867"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Encode code\n",
    "code_embeddings = model.encode(code_snippets)\n",
    "# Natural language query\n",
    "query = \"How do I implement a binary search algorithm?\"\n",
    "query_embedding = model.encode(query)\n",
    "# Find most relevant code\n",
    "similarities = util.cos_sim(query_embedding, code_embeddings)[0]\n",
    "best_match = np.argmax(similarities)\n",
    "print(f\"Most relevant code snippet:\\n{code_snippets[best_match]}\")\n"
   ],
   "id": "e0f9a20b05e86377"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Table Retrieval",
   "id": "8c046e73ccfbf729"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Tables in markdown format\n",
    "tables = [\n",
    "    \"\"\"\n",
    "    | Product | Q1 Revenue | Q2 Revenue |\n",
    "    |---------|-----------|-----------|\n",
    "    | Product A | $500K | $650K |\n",
    "    | Product B | $300K | $420K |\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    | Employee | Department | Salary |\n",
    "    |----------|-----------|--------|\n",
    "    | John Doe | Engineering | $120K |\n",
    "    | Jane Smith | Marketing | $95K |\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    | Country | Population | GDP |\n",
    "    |---------|-----------|-----|\n",
    "    | USA | 331M | $21T |\n",
    "    | China | 1.4B | $14T |\n",
    "    \"\"\"\n",
    "]"
   ],
   "id": "9bf09eb09ff72ee1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Encode tables\n",
    "table_embeddings = model.encode(tables)\n",
    "# Query for specific information\n",
    "query = \"What was the revenue growth for our products?\"\n",
    "query_embedding = model.encode(query)\n",
    "similarities = util.cos_sim(query_embedding, table_embeddings)[0]\n",
    "best_table = np.argmax(similarities)\n",
    "print(f\"Most relevant table:\\n{tables[best_table]}\")"
   ],
   "id": "b7b485850ee7c4e5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Batch Processing for Production",
   "id": "90f3753a509901b4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch"
   ],
   "id": "1566ef18cccc59ab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load model with GPU support\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = SentenceTransformer('ibm-granite/granite-embedding-english-r2', device=device,\n",
    "                            model_kwargs=model_kwargs)\n",
    "# Large batch of documents\n",
    "documents = corpus * 20\n",
    "# documents = generate_text(1024, num_docs=128)\n",
    "\n",
    "# Efficient batch encoding\n",
    "batch_size = 64\n",
    "all_embeddings = model.encode_document(\n",
    "    documents,\n",
    "    batch_size=batch_size,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_tensor=True,\n",
    "    normalize_embeddings=True  # For cosine similarity\n",
    ")\n",
    "# Save embeddings for later use\n",
    "torch.save(all_embeddings, 'document_embeddings.pt')\n",
    "# Load and search later\n",
    "embeddings = torch.load('document_embeddings.pt')\n",
    "query_emb = model.encode(query, convert_to_tensor=True)\n",
    "hits = util.semantic_search(query_emb, embeddings, top_k=10)\n",
    "print(hits)"
   ],
   "id": "18938662e458aba0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-09T19:35:13.228811Z",
     "start_time": "2025-10-09T19:35:13.224654Z"
    }
   },
   "cell_type": "code",
   "source": [
    "conversation_corpus = [\n",
    "    {'_id': '3',\n",
    "     'text': \"I'm not saying I don't like the idea of on-the-job training too, but you can't expect the company to do that. Training workers is not their job - they're building software. Perhaps educational systems in the U.S. (or their students) should worry a little about getting marketable skills in exchange for their massive investment in education, rather than getting out with thousands in student debt and then complaining that they aren't qualified to do anything.\",\n",
    "     },\n",
    "    {'_id': '31',\n",
    "    'text': \"So nothing preventing false ratings besides additional scrutiny from the market/investors, but there are some newer controls in place to prevent institutions from using them. Under the DFA banks can no longer solely rely on credit ratings as due diligence to buy a financial instrument, so that's a plus. The intent being that if financial institutions do their own leg work then *maybe* they'll figure out that a certain CDO is garbage or not.  Edit: lead in\",\n",
    "     },\n",
    "    {'_id': '56',\n",
    "    'text': \"You can never use a health FSA for individual health insurance premiums.  Moreover, FSA plan sponsors can limit what they are will to reimburse.  While you can't use a health FSA for premiums, you could previously use a 125 cafeteria plan to pay premiums, but it had to be a separate election from the health FSA. However, under N. 2013-54, even using a cafeteria plan to pay for indivdiual premiums is effectively prohibited.\",\n",
    "     },\n",
    "    {'_id': '59',\n",
    "    'text': 'Samsung created the LCD and other flat screen technology like OLED. a few years ago every flat screen came from Samsung factories and were reshelled. I think the 21 Hanns screen I am looking at now is Samsung and it is only a couple of years old. Samsung seem to be a good company.',\n",
    "     },\n",
    "    {'_id': '63',\n",
    "    'text': 'Here are the SEC requirements: The federal securities laws define the term accredited investor in   Rule 501 of Regulation D as: a bank, insurance company, registered investment company, business development company, or small business investment company; an employee benefit plan, within the meaning of the Employee Retirement Income Security Act, if a bank, insurance company, or   registered investment adviser makes the investment decisions, or if   the plan has total assets in excess of $5 million; a charitable organization, corporation, or partnership with assets exceeding $5 million; a director, executive officer, or general partner of the company selling the securities; a business in which all the equity owners are accredited investors; a natural person who has individual net worth, or joint net worth with the person’s spouse, that exceeds $1 million at the time of the   purchase, excluding the value of the primary residence of such person; a natural person with income exceeding $200,000 in each of the two most recent years or joint income with a spouse exceeding $300,000 for   those years and a reasonable expectation of the same income level in   the current year; or a trust with assets in excess of $5 million, not formed to acquire the securities offered, whose purchases a sophisticated person makes. No citizenship/residency requirements.',\n",
    "     },\n",
    "    {'_id': '100',\n",
    "    'text': '\"Only relevant to those with fantasy economy teams. Seriously, Rand\\'s fictional works never translate well into reality because, no matter how hard people try, that \"\"fiction\"\" element just can\\'t be ignored.  Test it yourself: Strip John Galt and his followers of everything they have which was created by or within the \"\"society\"\" they so revile, drop them in the desert -- and they\\'ll all be dead of exposure and starvation in less than two weeks because they will be naked, without tools and without food.  The only reason the libertarians get away with pushing their tripe as a rational philosophy is because no one will point out what it is wrong with their thinking. Why? Well, for most of my lifetime, their \"\"philosophy\"\" was considered nuttery in line with the John Birchers and so why bother. It\\'s only with the ascendency of these billionaire-funded politicians that this crap thinking has become acceptable, and even then, only to them.\"',\n",
    "     },\n",
    "    {'_id': '108',\n",
    "    'text': \"Futures contracts are a member of a larger class of financial assets called derivatives. Derivatives are called such because their payoffs depend on the price of other assets (financial or real). Other kinds of derivatives are call options, put options. Fixed income assets that mimic the behavior of derivatives are callable bonds, puttable bonds etc.  A futures contract is a contract that specifies the following: Just like with any other contract, there are two parties involved. One party commits to delivering the underlying asset to the other party on expiration date in exchange for the futures price. The other party commits to paying the futures price in exchange for the asset. There is no price that any of the two parties pay upfront to engage in the contract. The language used is so that the agent committing to receiving the delivery of the underlying asset is said to have bought the contract. The agent that commits to make the delivery is said to have sold the contract.  So answer your question, buying on June 1 a futures contract at the futures price of $100, with a maturity date on August 1 means you commit to paying $100 for the underlying asset on August 1. You don't have to pay anything upfront. Futures price is simply what the contract prescribes the underlying asset will exchange hands for.\",\n",
    "     },\n",
    "    {'_id': '125',\n",
    "    'text': 'This month when you join Scentsy you get a free defuser with your kit!   This has never been done before.  You also get spring / summer and Fall / Winter testers plus all your kit items!    Be your own boss!  You choose what hours you work, when and where you work them.   Join my Scentsy family today!  [Amanda C. Robar Scentsy Business ](http://www.amandacrobar.scentsy.ca)',\n",
    "     },\n",
    "    {'_id': '132',\n",
    "    'text': 'Whenever you pay or withdraw some fund from your account, paypal takes approx 3% of the current currency value along with the fees. i.e. If you are paying/withdraw 100 unit of US Dollars to British pounds and if the current convertion rate is 1$=0.82GBP, then consider reducing 3% of the actual currency rate. So, the approximate magnitude will be 0.82*97% (100-3=97) = 0.7954. So, 1$=0.7954GBP. This formula will not give you 100% accurate value but will help of course. Captain',\n",
    "     },\n",
    "    {'_id': '138',\n",
    "    'text': 'So you asked him in 2010 how he was gong to compete with DVD rental distributors like Netflix (which is what Netflix primarily was at the time) and Lovefilm and you were surprised that he was he said they were going to continue to compete as a DVD rental distributor just like the mentioned competitors?',\n",
    "     }\n",
    "    ]"
   ],
   "id": "9bb7e650b68d956c",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-09T19:42:42.656926Z",
     "start_time": "2025-10-09T19:42:42.652858Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# retriever = granite_retriever\n",
    "retriever = gemma_retriever"
   ],
   "id": "16d9b64a34c3690b",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-09T19:42:43.156902Z",
     "start_time": "2025-10-09T19:42:43.104253Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Concatenate conversation context\n",
    "conv_embeddings = retriever.encode_document([s['text'] for s in conversation_corpus], convert_to_tensor=True)"
   ],
   "id": "a159d6304cbeb8d3",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-09T19:41:41.131508Z",
     "start_time": "2025-10-09T19:41:41.128332Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run_query(query, corpus_embeddings=conv_embeddings, retriever=retriever, reranker=None):\n",
    "    results = search(corpus_embeddings=corpus_embeddings,\n",
    "                     query=query, retriever=retriever, reranker=reranker)\n",
    "    print(f\"Results for the query: '{query}'\")\n",
    "    for hit in results:\n",
    "        print(f\"Score: {hit['score']:.4f} | {conversation_corpus[hit['corpus_id']]['text'][:80]} [...]\")\n",
    "    print()"
   ],
   "id": "6c51d48ded9a2f9e",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-09T19:42:46.277880Z",
     "start_time": "2025-10-09T19:42:46.214332Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Search in your knowledge base\n",
    "query = \"Who invented the OLED screen?\"\n",
    "query1 = \"How real is Rand's writing?\"\n",
    "query2 = \"What's a futures contract?\"\n",
    "run_query(query)\n",
    "run_query(query1)\n",
    "run_query(query2)"
   ],
   "id": "6034870fb4afa8f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for the query: 'Who invented the OLED screen?'\n",
      "Score: 0.8672 | Samsung created the LCD and other flat screen technology like OLED. a few years  [...]\n",
      "Score: 0.6875 | So you asked him in 2010 how he was gong to compete with DVD rental distributors [...]\n",
      "Score: 0.6719 | This month when you join Scentsy you get a free defuser with your kit!   This ha [...]\n",
      "Score: 0.6523 | I'm not saying I don't like the idea of on-the-job training too, but you can't e [...]\n",
      "Score: 0.6523 | \"Only relevant to those with fantasy economy teams. Seriously, Rand's fictional  [...]\n",
      "\n",
      "Results for the query: 'How real is Rand's writing?'\n",
      "Score: 0.8633 | \"Only relevant to those with fantasy economy teams. Seriously, Rand's fictional  [...]\n",
      "Score: 0.6914 | So nothing preventing false ratings besides additional scrutiny from the market/ [...]\n",
      "Score: 0.6875 | Futures contracts are a member of a larger class of financial assets called deri [...]\n",
      "Score: 0.6836 | So you asked him in 2010 how he was gong to compete with DVD rental distributors [...]\n",
      "Score: 0.6797 | I'm not saying I don't like the idea of on-the-job training too, but you can't e [...]\n",
      "\n",
      "Results for the query: 'What's a futures contract?'\n",
      "Score: 0.9375 | Futures contracts are a member of a larger class of financial assets called deri [...]\n",
      "Score: 0.7344 | So nothing preventing false ratings besides additional scrutiny from the market/ [...]\n",
      "Score: 0.7305 | Here are the SEC requirements: The federal securities laws define the term accre [...]\n",
      "Score: 0.7227 | You can never use a health FSA for individual health insurance premiums.  Moreov [...]\n",
      "Score: 0.7227 | Whenever you pay or withdraw some fund from your account, paypal takes approx 3% [...]\n",
      "\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "19dc093275550c46"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T21:28:59.590754Z",
     "start_time": "2025-10-02T21:28:59.587847Z"
    }
   },
   "cell_type": "markdown",
   "source": [
    "### Text Generation Section\n",
    "Use this section to be able to quickly generate random pieces of text to use in the notebook above, if you don't have your own large corpus.\n",
    "\n",
    "This is how you can easily set up a similarity computation with one of the granite embedding model. Feel free to experiment with other models, of course :)."
   ],
   "id": "f3c26ef8c768dbaa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-09T15:12:04.103378Z",
     "start_time": "2025-10-09T15:12:04.101222Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "default_llm = \"ibm-granite/granite-4.0-h-micro\""
   ],
   "id": "7b0075032ac40976",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-09T15:12:06.994683Z",
     "start_time": "2025-10-09T15:12:05.238533Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_causal_lm_model(model_name, device=\"cuda\"):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    # drop device_map if running on CPU\n",
    "    granite_model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "    device_map=device)\n",
    "    return granite_model, tokenizer\n",
    "\n",
    "\n",
    "granite_model, tokenizer = create_causal_lm_model(default_llm)"
   ],
   "id": "d43400d953194ac2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The fast path is not available because on of `(selective_state_update, causal_conv1d_fn, causal_conv1d_update)` is None. Falling back to the naive implementation. To install follow https://github.com/state-spaces/mamba/#installation and https://github.com/Dao-AILab/causal-conv1d\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9dadf466a54d460486e440261b5c7f97"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 23.52 GiB of which 11.31 MiB is free. Process 3618285 has 386.00 MiB memory in use. Process 3618617 has 386.00 MiB memory in use. Process 3758166 has 3.74 GiB memory in use. Process 3776779 has 2.84 GiB memory in use. Process 1290329 has 9.03 GiB memory in use. Process 1721000 has 2.12 GiB memory in use. Including non-PyTorch memory, this process has 4.98 GiB memory in use. Of the allocated memory 4.49 GiB is allocated by PyTorch, and 36.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mOutOfMemoryError\u001B[39m                          Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[31]\u001B[39m\u001B[32m, line 9\u001B[39m\n\u001B[32m      4\u001B[39m     granite_model = AutoModelForCausalLM.from_pretrained(model_name,\n\u001B[32m      5\u001B[39m     device_map=device)\n\u001B[32m      6\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m granite_model, tokenizer\n\u001B[32m----> \u001B[39m\u001B[32m9\u001B[39m granite_model, tokenizer = \u001B[43mcreate_causal_lm_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdefault_llm\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[31]\u001B[39m\u001B[32m, line 4\u001B[39m, in \u001B[36mcreate_causal_lm_model\u001B[39m\u001B[34m(model_name, device)\u001B[39m\n\u001B[32m      2\u001B[39m tokenizer = AutoTokenizer.from_pretrained(model_name)\n\u001B[32m      3\u001B[39m \u001B[38;5;66;03m# drop device_map if running on CPU\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m4\u001B[39m granite_model = \u001B[43mAutoModelForCausalLM\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      5\u001B[39m \u001B[43m\u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      6\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m granite_model, tokenizer\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniforge3/envs/docu/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:604\u001B[39m, in \u001B[36m_BaseAutoModelClass.from_pretrained\u001B[39m\u001B[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[39m\n\u001B[32m    602\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m model_class.config_class == config.sub_configs.get(\u001B[33m\"\u001B[39m\u001B[33mtext_config\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[32m    603\u001B[39m         config = config.get_text_config()\n\u001B[32m--> \u001B[39m\u001B[32m604\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmodel_class\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    605\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43mmodel_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mhub_kwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m    606\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    607\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m    608\u001B[39m     \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mUnrecognized configuration class \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig.\u001B[34m__class__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m for this kind of AutoModel: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mcls\u001B[39m.\u001B[34m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m    609\u001B[39m     \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mModel type should be one of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[33m'\u001B[39m\u001B[33m, \u001B[39m\u001B[33m'\u001B[39m.join(c.\u001B[34m__name__\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mfor\u001B[39;00m\u001B[38;5;250m \u001B[39mc\u001B[38;5;250m \u001B[39m\u001B[38;5;129;01min\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28mcls\u001B[39m._model_mapping)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    610\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniforge3/envs/docu/lib/python3.12/site-packages/transformers/modeling_utils.py:288\u001B[39m, in \u001B[36mrestore_default_dtype.<locals>._wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    286\u001B[39m old_dtype = torch.get_default_dtype()\n\u001B[32m    287\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m288\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    289\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    290\u001B[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniforge3/envs/docu/lib/python3.12/site-packages/transformers/modeling_utils.py:5176\u001B[39m, in \u001B[36mPreTrainedModel.from_pretrained\u001B[39m\u001B[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001B[39m\n\u001B[32m   5166\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m dtype_orig \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m   5167\u001B[39m         torch.set_default_dtype(dtype_orig)\n\u001B[32m   5169\u001B[39m     (\n\u001B[32m   5170\u001B[39m         model,\n\u001B[32m   5171\u001B[39m         missing_keys,\n\u001B[32m   5172\u001B[39m         unexpected_keys,\n\u001B[32m   5173\u001B[39m         mismatched_keys,\n\u001B[32m   5174\u001B[39m         offload_index,\n\u001B[32m   5175\u001B[39m         error_msgs,\n\u001B[32m-> \u001B[39m\u001B[32m5176\u001B[39m     ) = \u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_load_pretrained_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   5177\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5178\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstate_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5179\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcheckpoint_files\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5180\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5181\u001B[39m \u001B[43m        \u001B[49m\u001B[43mignore_mismatched_sizes\u001B[49m\u001B[43m=\u001B[49m\u001B[43mignore_mismatched_sizes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5182\u001B[39m \u001B[43m        \u001B[49m\u001B[43msharded_metadata\u001B[49m\u001B[43m=\u001B[49m\u001B[43msharded_metadata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5183\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5184\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdisk_offload_folder\u001B[49m\u001B[43m=\u001B[49m\u001B[43moffload_folder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5185\u001B[39m \u001B[43m        \u001B[49m\u001B[43moffload_state_dict\u001B[49m\u001B[43m=\u001B[49m\u001B[43moffload_state_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5186\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5187\u001B[39m \u001B[43m        \u001B[49m\u001B[43mhf_quantizer\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhf_quantizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5188\u001B[39m \u001B[43m        \u001B[49m\u001B[43mkeep_in_fp32_regex\u001B[49m\u001B[43m=\u001B[49m\u001B[43mkeep_in_fp32_regex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5189\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdevice_mesh\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdevice_mesh\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5190\u001B[39m \u001B[43m        \u001B[49m\u001B[43mkey_mapping\u001B[49m\u001B[43m=\u001B[49m\u001B[43mkey_mapping\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5191\u001B[39m \u001B[43m        \u001B[49m\u001B[43mweights_only\u001B[49m\u001B[43m=\u001B[49m\u001B[43mweights_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   5192\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   5193\u001B[39m \u001B[38;5;66;03m# make sure token embedding weights are still tied if needed\u001B[39;00m\n\u001B[32m   5194\u001B[39m model.tie_weights()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniforge3/envs/docu/lib/python3.12/site-packages/transformers/modeling_utils.py:5639\u001B[39m, in \u001B[36mPreTrainedModel._load_pretrained_model\u001B[39m\u001B[34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001B[39m\n\u001B[32m   5636\u001B[39m         args_list = logging.tqdm(args_list, desc=\u001B[33m\"\u001B[39m\u001B[33mLoading checkpoint shards\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m   5638\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m args \u001B[38;5;129;01min\u001B[39;00m args_list:\n\u001B[32m-> \u001B[39m\u001B[32m5639\u001B[39m         _error_msgs, disk_offload_index, cpu_offload_index = \u001B[43mload_shard_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   5640\u001B[39m         error_msgs += _error_msgs\n\u001B[32m   5642\u001B[39m \u001B[38;5;66;03m# Adjust offloaded weights name and save if needed\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniforge3/envs/docu/lib/python3.12/site-packages/transformers/modeling_utils.py:946\u001B[39m, in \u001B[36mload_shard_file\u001B[39m\u001B[34m(args)\u001B[39m\n\u001B[32m    944\u001B[39m \u001B[38;5;66;03m# Skip it with fsdp on ranks other than 0\u001B[39;00m\n\u001B[32m    945\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (is_fsdp_enabled() \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_local_dist_rank_0() \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_quantized):\n\u001B[32m--> \u001B[39m\u001B[32m946\u001B[39m     disk_offload_index, cpu_offload_index = \u001B[43m_load_state_dict_into_meta_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    947\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmodel_to_load\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    948\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstate_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    949\u001B[39m \u001B[43m        \u001B[49m\u001B[43mshard_file\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    950\u001B[39m \u001B[43m        \u001B[49m\u001B[43mexpected_keys\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    951\u001B[39m \u001B[43m        \u001B[49m\u001B[43mreverse_key_renaming_mapping\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    952\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    953\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdisk_offload_folder\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdisk_offload_folder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    954\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdisk_offload_index\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdisk_offload_index\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    955\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcpu_offload_folder\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcpu_offload_folder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    956\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcpu_offload_index\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcpu_offload_index\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    957\u001B[39m \u001B[43m        \u001B[49m\u001B[43mhf_quantizer\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhf_quantizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    958\u001B[39m \u001B[43m        \u001B[49m\u001B[43mis_safetensors\u001B[49m\u001B[43m=\u001B[49m\u001B[43mis_offloaded_safetensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    959\u001B[39m \u001B[43m        \u001B[49m\u001B[43mkeep_in_fp32_regex\u001B[49m\u001B[43m=\u001B[49m\u001B[43mkeep_in_fp32_regex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    960\u001B[39m \u001B[43m        \u001B[49m\u001B[43munexpected_keys\u001B[49m\u001B[43m=\u001B[49m\u001B[43munexpected_keys\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    961\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdevice_mesh\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdevice_mesh\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    962\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    964\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m error_msgs, disk_offload_index, cpu_offload_index\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniforge3/envs/docu/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001B[39m, in \u001B[36mcontext_decorator.<locals>.decorate_context\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    117\u001B[39m \u001B[38;5;129m@functools\u001B[39m.wraps(func)\n\u001B[32m    118\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mdecorate_context\u001B[39m(*args, **kwargs):\n\u001B[32m    119\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[32m--> \u001B[39m\u001B[32m120\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniforge3/envs/docu/lib/python3.12/site-packages/transformers/modeling_utils.py:850\u001B[39m, in \u001B[36m_load_state_dict_into_meta_model\u001B[39m\u001B[34m(model, state_dict, shard_file, expected_keys, reverse_renaming_mapping, device_map, disk_offload_folder, disk_offload_index, cpu_offload_folder, cpu_offload_index, hf_quantizer, is_safetensors, keep_in_fp32_regex, unexpected_keys, device_mesh)\u001B[39m\n\u001B[32m    847\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m is_fsdp_enabled():\n\u001B[32m    848\u001B[39m         param_device = \u001B[33m\"\u001B[39m\u001B[33mcpu\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m is_local_dist_rank_0() \u001B[38;5;28;01melse\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mmeta\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m--> \u001B[39m\u001B[32m850\u001B[39m     _load_parameter_into_model(model, param_name, \u001B[43mparam\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparam_device\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[32m    852\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    853\u001B[39m     \u001B[38;5;66;03m# TODO naming is stupid it loads it as well\u001B[39;00m\n\u001B[32m    854\u001B[39m     hf_quantizer.create_quantized_param(\n\u001B[32m    855\u001B[39m         model, param, param_name, param_device, state_dict, unexpected_keys\n\u001B[32m    856\u001B[39m     )\n",
      "\u001B[31mOutOfMemoryError\u001B[39m: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 23.52 GiB of which 11.31 MiB is free. Process 3618285 has 386.00 MiB memory in use. Process 3618617 has 386.00 MiB memory in use. Process 3758166 has 3.74 GiB memory in use. Process 3776779 has 2.84 GiB memory in use. Process 1290329 has 9.03 GiB memory in use. Process 1721000 has 2.12 GiB memory in use. Including non-PyTorch memory, this process has 4.98 GiB memory in use. Of the allocated memory 4.49 GiB is allocated by PyTorch, and 36.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def generate_text(size=1024, num_docs = 1000, device=\"cuda\"):\n",
    "    model.eval()\n",
    "    # change input text as desired\n",
    "    chat = [\n",
    "        { \"role\": \"user\", \"content\": \"Please generate creative text.\" },\n",
    "    ]\n",
    "    chat = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "    # tokenize the text\n",
    "    input_tokens = tokenizer(chat, return_tensors=\"pt\").to(device)\n",
    "    # generate output tokens\n",
    "    output = []\n",
    "    for _ in tqdm(range(num_docs), desc=\"Generating text\"):\n",
    "        o = granite_model.generate(**input_tokens,\n",
    "                                   max_new_tokens=size)\n",
    "        # decode output tokens into text\n",
    "        response_text = tokenizer.batch_decode(o, skip_special_tokens=False)[0]\n",
    "        assistant_turn_marker = \"<|start_of_role|>assistant\"\n",
    "        if assistant_turn_marker in response_text:\n",
    "            # Get the text after the last assistant turn marker\n",
    "            new_assistant_turn = response_text.rsplit(assistant_turn_marker, 1)[-1].strip()\n",
    "            # Clean up any remaining tokens or unwanted text\n",
    "            final_response = new_assistant_turn.replace(\"<|end_of_role|>\", \"\").strip()\n",
    "        else:\n",
    "            final_response = response_text.strip()\n",
    "        output.append(final_response)\n",
    "    # print output\n",
    "    # print(output[0])\n",
    "    return output[0] if len(output) == 1 else output"
   ],
   "id": "58b678c4aac9fef6",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 3,
   "source": "docs = generate_text(size=128, num_docs=10)",
   "id": "807f1105b25782f9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Generate text using LMStudio\n",
    ".. or any other OpenAI-based generator"
   ],
   "id": "c072002f8141fcd5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "!pip install openai",
   "id": "1dc67d1dd61e51ce",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from openai import OpenAI\n",
    "from tqdm.auto import tqdm"
   ],
   "id": "9bb511b8a34bc8a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Configuration constants\n",
    "LM_STUDIO_BASE_URL = \"http://localhost:1234/v1\"\n",
    "LM_STUDIO_API_KEY = \"not-needed\"\n",
    "MODEL_NAME = \"local-model\"\n",
    "SYSTEM_MESSAGE = \"You are a creative assistant.\"\n",
    "DEFAULT_TEMPERATURE = 0.7\n",
    "\n",
    "# Point to your local LM Studio server\n",
    "client = OpenAI(base_url=LM_STUDIO_BASE_URL, api_key=LM_STUDIO_API_KEY)\n",
    "\n",
    "\n",
    "def generate_text_lmstudio(size=1024, num_docs=1000):\n",
    "    # generate output tokens\n",
    "    output = []\n",
    "    for _ in tqdm(range(num_docs), desc=\"Generating text\"):\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL_NAME,  # LM Studio ignores this, uses loaded model\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": SYSTEM_MESSAGE},\n",
    "                {\"role\": \"user\", \"content\": f\"Generate an interesting story of at most {size} tokens.\"}\n",
    "            ],\n",
    "            temperature=DEFAULT_TEMPERATURE,\n",
    "            max_tokens=size\n",
    "        )\n",
    "        output.append(response.choices[0].message.content)\n",
    "    return output\n",
    "    # print(output[0])#%%\n",
    "docs = generate_text_lmstudio(size=512, num_docs=10)"
   ],
   "id": "e0aacf78f9282048",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Putting it all together in one class\n",
    "This class makes all the previous code snippets easier to run\n",
    "\n"
   ],
   "id": "47dfce66234611b2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-09T19:37:22.213737Z",
     "start_time": "2025-10-09T19:37:22.200056Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import List, Dict, Optional, Union\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder, util\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "DEFAULT_RETRIEVER_MODEL = 'ibm-granite/granite-embedding-english-r2'\n",
    "DEFAULT_RERANKER_MODEL = 'ibm-granite/granite-embedding-reranker-english-r2'\n",
    "DEFAULT_GEMMA_MODEL = \"google/embeddinggemma-300m\"\n",
    "DEFAULT_EMBEDDING_DIM = 768\n",
    "DEFAULT_TOP_K = 20\n",
    "DEFAULT_TOP_RESULTS = 5\n",
    "\n",
    "\n",
    "class SemanticSearchManager:\n",
    "    \"\"\"\n",
    "    A comprehensive semantic search system that combines retrieval and reranking\n",
    "    for enhanced search accuracy and performance.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 retriever_model: str = DEFAULT_RETRIEVER_MODEL,\n",
    "                 reranker_model: str = DEFAULT_RERANKER_MODEL,\n",
    "                 use_gpu: bool = True):\n",
    "        \"\"\"\n",
    "        Initialize the semantic search manager with retriever and reranker models.\n",
    "\n",
    "        Args:\n",
    "            retriever_model: Name of the SentenceTransformer model for retrieval\n",
    "            reranker_model: Name of the CrossEncoder model for reranking\n",
    "            use_gpu: Whether to use GPU acceleration if available\n",
    "        \"\"\"\n",
    "        self.retriever_model_name = retriever_model\n",
    "        self.reranker_model_name = reranker_model\n",
    "        self.corpus_embeddings = None\n",
    "        self.corpus = None\n",
    "\n",
    "        # Configure model kwargs based on hardware availability\n",
    "        self.model_kwargs = self._configure_model_kwargs(use_gpu)\n",
    "\n",
    "        # Initialize models\n",
    "        self.retriever = self._load_retriever()\n",
    "        self.reranker = self._load_reranker()\n",
    "\n",
    "    def _configure_model_kwargs(self, use_gpu: bool) -> Dict:\n",
    "        \"\"\"Configure model parameters based on hardware availability.\"\"\"\n",
    "        if use_gpu and torch.cuda.is_available():\n",
    "            return {\n",
    "                \"dtype\": torch.bfloat16,\n",
    "                'attn_implementation': 'flash_attention_2'\n",
    "            }\n",
    "        return {}\n",
    "\n",
    "    def _load_retriever(self) -> SentenceTransformer:\n",
    "        \"\"\"Load and configure the retriever model.\"\"\"\n",
    "        return SentenceTransformer(self.retriever_model_name,\n",
    "                                   model_kwargs=self.model_kwargs,\n",
    "                                   device=SemanticSearchManager._get_device())\n",
    "\n",
    "    def _load_reranker(self) -> Optional[CrossEncoder]:\n",
    "        \"\"\"Load and configure the reranker model.\"\"\"\n",
    "        try:\n",
    "            return CrossEncoder(self.reranker_model_name,\n",
    "                                model_kwargs=self.model_kwargs,\n",
    "                                device=SemanticSearchManager._get_device())\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not load reranker model: {e}\")\n",
    "            return None\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_device():\n",
    "        device = ('cuda' if torch.cuda.is_available() else\n",
    "                  \"mps\" if torch.backends.mps.is_available() else\n",
    "                  'cpu'\n",
    "                  )\n",
    "        return device\n",
    "\n",
    "    def encode_corpus(self, corpus: List[str], show_progress: bool = True):\n",
    "        \"\"\"\n",
    "        Encode the document corpus for semantic search.\n",
    "\n",
    "        Args:\n",
    "            corpus: List of documents to encode\n",
    "            show_progress: Whether to show encoding progress\n",
    "        \"\"\"\n",
    "        self.corpus = [self.get_text(s) for s in corpus]\n",
    "        self.corpus_embeddings = self.retriever.encode_document(\n",
    "            self.corpus,\n",
    "            convert_to_tensor=True,\n",
    "            show_progress_bar=show_progress\n",
    "        )\n",
    "        return self.corpus_embeddings\n",
    "\n",
    "    def encode_query(self, query: str) -> torch.Tensor:\n",
    "        \"\"\"Encode a single query for semantic search.\"\"\"\n",
    "        return self.retriever.encode_query(query, convert_to_tensor=True)\n",
    "\n",
    "    def search(self,\n",
    "               query: str,\n",
    "               top_k: int = DEFAULT_TOP_K,\n",
    "               top_results: int = DEFAULT_TOP_RESULTS,\n",
    "               use_reranker: bool = True,\n",
    "               print_results: bool=False,\n",
    "               max_size: int=-1) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Perform semantic search with optional reranking.\n",
    "\n",
    "        Args:\n",
    "            query: Search query string\n",
    "            top_k: Number of candidates to retrieve initially\n",
    "            top_results: Final number of results to return\n",
    "            use_reranker: Whether to use reranker for final ranking\n",
    "\n",
    "        Returns:\n",
    "            List of search results with scores and content\n",
    "        \"\"\"\n",
    "        if self.corpus_embeddings is None:\n",
    "            raise ValueError(\"Corpus must be encoded first using encode_corpus()\")\n",
    "\n",
    "        # Step 1: Retrieve top-k candidates\n",
    "        query_embedding = self.encode_query(query)\n",
    "        hits = util.semantic_search(query_embedding, self.corpus_embeddings, top_k=top_k)[0]\n",
    "\n",
    "        # Step 2: Optionally rerank with cross-encoder\n",
    "        if use_reranker and self.reranker is not None:\n",
    "            hits = self._rerank_results(query, hits)\n",
    "\n",
    "        if print_results:\n",
    "            self.print_results(query, hits[:top_results], max_size=max_size)\n",
    "\n",
    "        return hits[:top_results]\n",
    "\n",
    "    def _rerank_results(self, query: str, hits: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Apply reranking to search results.\"\"\"\n",
    "        cross_input = [[query, self.corpus[hit['corpus_id']]] for hit in hits]\n",
    "        cross_scores = self.reranker.predict(cross_input)\n",
    "\n",
    "        # Update hits with reranking scores\n",
    "        for idx, score in enumerate(cross_scores):\n",
    "            hits[idx]['rerank_score'] = score\n",
    "\n",
    "        # Sort by reranker scores\n",
    "        return sorted(hits, key=lambda x: x['rerank_score'], reverse=True)\n",
    "\n",
    "    def get_text(self, item):\n",
    "        if isinstance(item, str):\n",
    "            return item\n",
    "        elif isinstance(item, dict) and 'text' in item:\n",
    "            return item['text']\n",
    "\n",
    "    def print_results(self, query: str, results, top_results: int = DEFAULT_TOP_RESULTS,\n",
    "                      max_size=-1):\n",
    "\n",
    "        print(f\"Results for the query: '{query}'\")\n",
    "        for hit in results:\n",
    "            txt = self.get_text(self.corpus[hit['corpus_id']])\n",
    "            if max_size > 0 and len(txt) > max_size:\n",
    "                txt = f\"{txt[:max_size]} [...]\"\n",
    "            print(f\"\\tScore: {hit['score']:.4f} | {txt}\")\n",
    "        print()\n",
    "\n",
    "    def simple_similarity_search(self, query: str, documents: List[str]) -> Dict:\n",
    "        \"\"\"\n",
    "        Perform simple cosine similarity search without corpus pre-encoding.\n",
    "        Useful for one-time searches on small document sets.\n",
    "        \"\"\"\n",
    "        query_embedding = self.encode_query(query)\n",
    "        _ = self.encode_corpus(documents)\n",
    "\n",
    "        similarities = util.cos_sim(query_embedding, self.corpus_embeddings)\n",
    "        best_idx = torch.argmax(similarities)\n",
    "\n",
    "        return {\n",
    "            'similarities': similarities,\n",
    "            'best_match_index': int(best_idx),\n",
    "            'best_match': documents[best_idx],\n",
    "            'best_score': float(similarities[0][best_idx])\n",
    "        }\n"
   ],
   "id": "55ca446f42e31e0a",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-09T19:37:28.202899Z",
     "start_time": "2025-10-09T19:37:24.355896Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example: Simple similarity search\n",
    "def run_experiment(manager):\n",
    "    documents = [\n",
    "        \"Granite models are designed for enterprise applications\",\n",
    "        \"Information retrieval systems need fast and accurate embeddings\",\n",
    "        \"Machine learning models can process natural language\"\n",
    "    ]\n",
    "\n",
    "    query = \"What's the purpose of the granite models?\"\n",
    "    print(\"=\" * 80, \"\\n\", f\"Running with {manager.retriever_model_name}\", \"=\" * 80)\n",
    "    manager.encode_corpus(documents)\n",
    "    manager.search(query, top_k=5, use_reranker=False, print_results=True, max_size=-1)\n",
    "\n",
    "    manager.encode_corpus(conversation_corpus)\n",
    "    query = \"Who invented the OLED screen?\"\n",
    "    query1 = \"How real is Rand's writing?\"\n",
    "    manager.search(query, top_k=5, print_results=True, max_size=80, use_reranker=False)\n",
    "    manager.search(query1, top_k=5, print_results=True, max_size=80, use_reranker=False)\n",
    "\n",
    "search_manager = SemanticSearchManager()\n",
    "run_experiment(search_manager)\n",
    "\n",
    "gemma_manager = SemanticSearchManager(retriever_model=DEFAULT_GEMMA_MODEL)\n",
    "run_experiment(gemma_manager)\n"
   ],
   "id": "e7a7dd861c650a49",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================ \n",
      " Running with ibm-granite/granite-embedding-english-r2 ================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cc0b985b9b8d45629c1c2064d08e76ba"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for the query: 'What's the purpose of the granite models?'\n",
      "\tScore: 0.8750 | Granite models are designed for enterprise applications\n",
      "\tScore: 0.7344 | Machine learning models can process natural language\n",
      "\tScore: 0.7188 | Information retrieval systems need fast and accurate embeddings\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3e7faa8744eb4bb19edd5ce2e314ec00"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for the query: 'Who invented the OLED screen?'\n",
      "\tScore: 0.8672 | Samsung created the LCD and other flat screen technology like OLED. a few years  [...]\n",
      "\tScore: 0.6875 | So you asked him in 2010 how he was gong to compete with DVD rental distributors [...]\n",
      "\tScore: 0.6719 | This month when you join Scentsy you get a free defuser with your kit!   This ha [...]\n",
      "\tScore: 0.6523 | I'm not saying I don't like the idea of on-the-job training too, but you can't e [...]\n",
      "\tScore: 0.6523 | \"Only relevant to those with fantasy economy teams. Seriously, Rand's fictional  [...]\n",
      "\n",
      "Results for the query: 'How real is Rand's writing?'\n",
      "\tScore: 0.8633 | \"Only relevant to those with fantasy economy teams. Seriously, Rand's fictional  [...]\n",
      "\tScore: 0.6914 | So nothing preventing false ratings besides additional scrutiny from the market/ [...]\n",
      "\tScore: 0.6875 | Futures contracts are a member of a larger class of financial assets called deri [...]\n",
      "\tScore: 0.6836 | So you asked him in 2010 how he was gong to compete with DVD rental distributors [...]\n",
      "\tScore: 0.6797 | I'm not saying I don't like the idea of on-the-job training too, but you can't e [...]\n",
      "\n",
      "================================================================================ \n",
      " Running with google/embeddinggemma-300m ================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bf9613e49fea4e99af86000f8d7359a3"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for the query: 'What's the purpose of the granite models?'\n",
      "\tScore: 0.6719 | Granite models are designed for enterprise applications\n",
      "\tScore: 0.2334 | Machine learning models can process natural language\n",
      "\tScore: 0.1699 | Information retrieval systems need fast and accurate embeddings\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "50244d6df6d24f498f9a09c11e3d5c06"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for the query: 'Who invented the OLED screen?'\n",
      "\tScore: 0.4961 | Samsung created the LCD and other flat screen technology like OLED. a few years  [...]\n",
      "\tScore: 0.1465 | I'm not saying I don't like the idea of on-the-job training too, but you can't e [...]\n",
      "\tScore: 0.1025 | Here are the SEC requirements: The federal securities laws define the term accre [...]\n",
      "\tScore: 0.0996 | So you asked him in 2010 how he was gong to compete with DVD rental distributors [...]\n",
      "\tScore: 0.0830 | This month when you join Scentsy you get a free defuser with your kit!   This ha [...]\n",
      "\n",
      "Results for the query: 'How real is Rand's writing?'\n",
      "\tScore: 0.5703 | \"Only relevant to those with fantasy economy teams. Seriously, Rand's fictional  [...]\n",
      "\tScore: 0.1709 | So nothing preventing false ratings besides additional scrutiny from the market/ [...]\n",
      "\tScore: 0.1396 | Samsung created the LCD and other flat screen technology like OLED. a few years  [...]\n",
      "\tScore: 0.1309 | Here are the SEC requirements: The federal securities laws define the term accre [...]\n",
      "\tScore: 0.1201 | I'm not saying I don't like the idea of on-the-job training too, but you can't e [...]\n",
      "\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-09T19:01:19.379593Z",
     "start_time": "2025-10-09T19:01:19.375730Z"
    }
   },
   "cell_type": "code",
   "source": "result",
   "id": "63db39ac6f2469e6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'similarities': tensor([[0.8750, 0.7188, 0.7344]], device='cuda:0', dtype=torch.bfloat16),\n",
       " 'best_match_index': 0,\n",
       " 'best_match': 'Granite models are designed for enterprise applications',\n",
       " 'best_score': 0.875}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "75c9160c856792ad"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
