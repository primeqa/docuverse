{
 "cells": [
  {
   "cell_type": "code",
   "id": "f513f4f0bbc79b25",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T18:14:21.583981Z",
     "start_time": "2025-05-16T18:14:16.272012Z"
    }
   },
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from docuverse.utils import open_stream, read_config_file\n",
    "import pickle\n",
    "from tqdm.autonotebook import tqdm, trange\n",
    "from docuverse.utils.text_tiler import TextTiler\n",
    "from docuverse.engines.search_data import SearchData\n",
    "from docuverse.engines.data_template import DataTemplate\n",
    "import os"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T18:40:37.782731Z",
     "start_time": "2025-05-16T18:40:22.249965Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# model_name = \"thenlper/gte-small\"\n",
    "# model_name = \"/home/raduf/mnlp-models-rag/06302024/slate.30m.english.rtrvr\"\n",
    "model_name = \"ibm-granite/granite-embedding-30m-english\"\n",
    "model = SentenceTransformer(model_name)\n",
    "max_size = 400\n",
    "stride = 100\n",
    "tiler = TextTiler(max_doc_length=max_size, stride=stride, tokenizer=model.tokenizer, aligned_on_sentences=False, trim_text_to=400)"
   ],
   "id": "eab9fc0cb1a75545",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T18:44:51.340647Z",
     "start_time": "2025-05-16T18:44:51.338101Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# datadir=\"data/clapnq_small\"\n",
    "datadir=\"notebooks/data/long_example\""
   ],
   "id": "7bd8afacca249937",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T18:44:52.608801Z",
     "start_time": "2025-05-16T18:44:52.599789Z"
    }
   },
   "cell_type": "code",
   "source": [
    "file = \"Long programmatic PDF.md\"\n",
    "if file == \"passages.tsv\":\n",
    "     import pandas as pd\n",
    "     data = pd.read_csv(os.path.join(datadir, \"passages.tsv\"), sep=\"\\t\")\n",
    "     sents = data['text']\n",
    "elif file == \"Long programmatic PDF.md\":\n",
    "    sents = [\"\".join(open_stream(os.path.join(datadir, \"Long programmatic PDF.md\")).readlines())]"
   ],
   "id": "1fc43649f9f01481",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T18:45:05.687822Z",
     "start_time": "2025-05-16T18:45:05.684724Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "24d6cdde108e525",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "tiler.tokenizer.cls_token",
   "id": "ec8a605ec1225711",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "tiled_data = [tiler.create_tiles(i, txt, '', max_doc_size=512, stride=100, title_handling=\"none\") for i, txt in enumerate(sents)]",
   "id": "5cd5bf94ce691d6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "tiled_data[0][:10]",
   "id": "b34e7db6efcedf29",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Run the tiler in parallel",
   "id": "a03833d6b51b00af"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from docuverse.utils import parallel_process\n",
    "\n",
    "def proc_text(item):\n",
    "     res = tiler.create_tiles(item['id'], item['text'], item['title'])\n",
    "     item['result'] = res\n",
    "     return item\n",
    "\n",
    "res = parallel_process(proc_text, [{'id': i, 'text': txt, 'title': f\"Title for item {i}\"} for i, txt in enumerate(sents[:100])], num_threads=10)"
   ],
   "id": "284c76e773746a13",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T18:46:33.478889Z",
     "start_time": "2025-05-16T18:46:32.564673Z"
    }
   },
   "cell_type": "code",
   "source": "res=tiler.create_tiles(id_='text0', text=sents[0])",
   "id": "cf0f17f2bfac7e4",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "ename": "OverflowError",
     "evalue": "can't convert negative int to unsigned",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mOverflowError\u001B[39m                             Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[14]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m res=\u001B[43mtiler\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcreate_tiles\u001B[49m\u001B[43m(\u001B[49m\u001B[43mid_\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mtext0\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtext\u001B[49m\u001B[43m=\u001B[49m\u001B[43msents\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/sandbox2/docuverse/docuverse/utils/text_tiler.py:132\u001B[39m, in \u001B[36mTextTiler.create_tiles\u001B[39m\u001B[34m(self, id_, text, title, max_doc_size, stride, remove_url, normalize_text, title_handling, template)\u001B[39m\n\u001B[32m    129\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    130\u001B[39m     maxl = max_doc_size  \u001B[38;5;66;03m# - title_len\u001B[39;00m\n\u001B[32m    131\u001B[39m     psgs, inds, added_titles = \\\n\u001B[32m--> \u001B[39m\u001B[32m132\u001B[39m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43msplit_text\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_length\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmaxl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtitle\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtitle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    133\u001B[39m \u001B[43m                        \u001B[49m\u001B[43mstride\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtitle_handling\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtitle_handling\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    134\u001B[39m \u001B[43m                        \u001B[49m\u001B[43mtitle_in_text\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtitle_in_text\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    135\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m pi, (p, index, added_title) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[38;5;28mzip\u001B[39m(psgs, inds, added_titles)):\n\u001B[32m    136\u001B[39m         itm.update({\n\u001B[32m    137\u001B[39m             \u001B[33m'\u001B[39m\u001B[33mid\u001B[39m\u001B[33m'\u001B[39m: \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mid_\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m-\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mindex[\u001B[32m0\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m-\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mindex[\u001B[32m1\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    138\u001B[39m             \u001B[33m'\u001B[39m\u001B[33mtext\u001B[39m\u001B[33m'\u001B[39m: (\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtitle\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mp\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m    139\u001B[39m                      \u001B[38;5;28;01mif\u001B[39;00m added_title\n\u001B[32m    140\u001B[39m                      \u001B[38;5;28;01melse\u001B[39;00m p)\n\u001B[32m    141\u001B[39m         })\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/sandbox2/docuverse/docuverse/utils/text_tiler.py:221\u001B[39m, in \u001B[36mTextTiler.split_text\u001B[39m\u001B[34m(self, text, tokenizer, title, max_length, stride, language_code, title_handling, title_in_text)\u001B[39m\n\u001B[32m    218\u001B[39m     \u001B[38;5;28mself\u001B[39m._init_nlp(language_code=language_code)\n\u001B[32m    220\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.text_trim_to \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m221\u001B[39m     text, parsed_text, max_num_sentences = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtrim_text\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtitle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtitle_in_text\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtitle_in_text\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    222\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m max_length \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    223\u001B[39m     tok_len = \u001B[38;5;28mself\u001B[39m.get_tokenized_length(get_expanded_text(text=text, title=title,\n\u001B[32m    224\u001B[39m                                                           title_handling=title_handling,\n\u001B[32m    225\u001B[39m                                                           title_in_text=title_in_text)\n\u001B[32m    226\u001B[39m                                         )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/sandbox2/docuverse/docuverse/utils/text_tiler.py:497\u001B[39m, in \u001B[36mTextTiler.trim_text\u001B[39m\u001B[34m(self, text, title, title_in_text)\u001B[39m\n\u001B[32m    495\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m text, parsed_text, num_sentences\n\u001B[32m    496\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m497\u001B[39m     tokenized_text = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_length\u001B[49m\u001B[43m=\u001B[49m\u001B[43m-\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m    498\u001B[39m     total_size = \u001B[32m0\u001B[39m\n\u001B[32m    499\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m tokenized_text[-\u001B[32m1\u001B[39m][\u001B[32m1\u001B[39m] < \u001B[38;5;28mself\u001B[39m.text_trim_to:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniforge3/envs/docuverse12/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2877\u001B[39m, in \u001B[36mPreTrainedTokenizerBase.__call__\u001B[39m\u001B[34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001B[39m\n\u001B[32m   2875\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m._in_target_context_manager:\n\u001B[32m   2876\u001B[39m         \u001B[38;5;28mself\u001B[39m._switch_to_input_mode()\n\u001B[32m-> \u001B[39m\u001B[32m2877\u001B[39m     encodings = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_one\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtext_pair\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtext_pair\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mall_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2878\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m text_target \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m   2879\u001B[39m     \u001B[38;5;28mself\u001B[39m._switch_to_target_mode()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniforge3/envs/docuverse12/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2987\u001B[39m, in \u001B[36mPreTrainedTokenizerBase._call_one\u001B[39m\u001B[34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001B[39m\n\u001B[32m   2965\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.batch_encode_plus(\n\u001B[32m   2966\u001B[39m         batch_text_or_text_pairs=batch_text_or_text_pairs,\n\u001B[32m   2967\u001B[39m         add_special_tokens=add_special_tokens,\n\u001B[32m   (...)\u001B[39m\u001B[32m   2984\u001B[39m         **kwargs,\n\u001B[32m   2985\u001B[39m     )\n\u001B[32m   2986\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m2987\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mencode_plus\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   2988\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtext\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2989\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtext_pair\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtext_pair\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2990\u001B[39m \u001B[43m        \u001B[49m\u001B[43madd_special_tokens\u001B[49m\u001B[43m=\u001B[49m\u001B[43madd_special_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2991\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpadding\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2992\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtruncation\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtruncation\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2993\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmax_length\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmax_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2994\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstride\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2995\u001B[39m \u001B[43m        \u001B[49m\u001B[43mis_split_into_words\u001B[49m\u001B[43m=\u001B[49m\u001B[43mis_split_into_words\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2996\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpad_to_multiple_of\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpad_to_multiple_of\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2997\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpadding_side\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpadding_side\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2998\u001B[39m \u001B[43m        \u001B[49m\u001B[43mreturn_tensors\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_tensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2999\u001B[39m \u001B[43m        \u001B[49m\u001B[43mreturn_token_type_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_token_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3000\u001B[39m \u001B[43m        \u001B[49m\u001B[43mreturn_attention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3001\u001B[39m \u001B[43m        \u001B[49m\u001B[43mreturn_overflowing_tokens\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_overflowing_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3002\u001B[39m \u001B[43m        \u001B[49m\u001B[43mreturn_special_tokens_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_special_tokens_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3003\u001B[39m \u001B[43m        \u001B[49m\u001B[43mreturn_offsets_mapping\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_offsets_mapping\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3004\u001B[39m \u001B[43m        \u001B[49m\u001B[43mreturn_length\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3005\u001B[39m \u001B[43m        \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m=\u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3006\u001B[39m \u001B[43m        \u001B[49m\u001B[43msplit_special_tokens\u001B[49m\u001B[43m=\u001B[49m\u001B[43msplit_special_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3007\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3008\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniforge3/envs/docuverse12/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3063\u001B[39m, in \u001B[36mPreTrainedTokenizerBase.encode_plus\u001B[39m\u001B[34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001B[39m\n\u001B[32m   3053\u001B[39m \u001B[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001B[39;00m\n\u001B[32m   3054\u001B[39m padding_strategy, truncation_strategy, max_length, kwargs = \u001B[38;5;28mself\u001B[39m._get_padding_truncation_strategies(\n\u001B[32m   3055\u001B[39m     padding=padding,\n\u001B[32m   3056\u001B[39m     truncation=truncation,\n\u001B[32m   (...)\u001B[39m\u001B[32m   3060\u001B[39m     **kwargs,\n\u001B[32m   3061\u001B[39m )\n\u001B[32m-> \u001B[39m\u001B[32m3063\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_encode_plus\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   3064\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtext\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3065\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtext_pair\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtext_pair\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3066\u001B[39m \u001B[43m    \u001B[49m\u001B[43madd_special_tokens\u001B[49m\u001B[43m=\u001B[49m\u001B[43madd_special_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3067\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpadding_strategy\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpadding_strategy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3068\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtruncation_strategy\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtruncation_strategy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3069\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmax_length\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmax_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3070\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstride\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3071\u001B[39m \u001B[43m    \u001B[49m\u001B[43mis_split_into_words\u001B[49m\u001B[43m=\u001B[49m\u001B[43mis_split_into_words\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3072\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpad_to_multiple_of\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpad_to_multiple_of\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3073\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpadding_side\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpadding_side\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3074\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_tensors\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_tensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3075\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_token_type_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_token_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3076\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_attention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3077\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_overflowing_tokens\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_overflowing_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3078\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_special_tokens_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_special_tokens_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3079\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_offsets_mapping\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_offsets_mapping\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3080\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_length\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3081\u001B[39m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m=\u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3082\u001B[39m \u001B[43m    \u001B[49m\u001B[43msplit_special_tokens\u001B[49m\u001B[43m=\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m.\u001B[49m\u001B[43mpop\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43msplit_special_tokens\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43msplit_special_tokens\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3083\u001B[39m \u001B[43m    \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   3084\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniforge3/envs/docuverse12/lib/python3.12/site-packages/transformers/models/roberta/tokenization_roberta_fast.py:227\u001B[39m, in \u001B[36mRobertaTokenizerFast._encode_plus\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m    220\u001B[39m is_split_into_words = kwargs.get(\u001B[33m\"\u001B[39m\u001B[33mis_split_into_words\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[32m    222\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m.add_prefix_space \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_split_into_words, (\n\u001B[32m    223\u001B[39m     \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mYou need to instantiate \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.\u001B[34m__class__\u001B[39m.\u001B[34m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m with add_prefix_space=True \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    224\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mto use it with pretokenized inputs.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    225\u001B[39m )\n\u001B[32m--> \u001B[39m\u001B[32m227\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_encode_plus\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniforge3/envs/docuverse12/lib/python3.12/site-packages/transformers/tokenization_utils_fast.py:613\u001B[39m, in \u001B[36mPreTrainedTokenizerFast._encode_plus\u001B[39m\u001B[34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001B[39m\n\u001B[32m    589\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_encode_plus\u001B[39m(\n\u001B[32m    590\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m    591\u001B[39m     text: Union[TextInput, PreTokenizedInput],\n\u001B[32m   (...)\u001B[39m\u001B[32m    610\u001B[39m     **kwargs,\n\u001B[32m    611\u001B[39m ) -> BatchEncoding:\n\u001B[32m    612\u001B[39m     batched_input = [(text, text_pair)] \u001B[38;5;28;01mif\u001B[39;00m text_pair \u001B[38;5;28;01melse\u001B[39;00m [text]\n\u001B[32m--> \u001B[39m\u001B[32m613\u001B[39m     batched_output = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_batch_encode_plus\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    614\u001B[39m \u001B[43m        \u001B[49m\u001B[43mbatched_input\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    615\u001B[39m \u001B[43m        \u001B[49m\u001B[43mis_split_into_words\u001B[49m\u001B[43m=\u001B[49m\u001B[43mis_split_into_words\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    616\u001B[39m \u001B[43m        \u001B[49m\u001B[43madd_special_tokens\u001B[49m\u001B[43m=\u001B[49m\u001B[43madd_special_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    617\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpadding_strategy\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpadding_strategy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    618\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtruncation_strategy\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtruncation_strategy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    619\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmax_length\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmax_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    620\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstride\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    621\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpad_to_multiple_of\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpad_to_multiple_of\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    622\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpadding_side\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpadding_side\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    623\u001B[39m \u001B[43m        \u001B[49m\u001B[43mreturn_tensors\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_tensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    624\u001B[39m \u001B[43m        \u001B[49m\u001B[43mreturn_token_type_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_token_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    625\u001B[39m \u001B[43m        \u001B[49m\u001B[43mreturn_attention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    626\u001B[39m \u001B[43m        \u001B[49m\u001B[43mreturn_overflowing_tokens\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_overflowing_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    627\u001B[39m \u001B[43m        \u001B[49m\u001B[43mreturn_special_tokens_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_special_tokens_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    628\u001B[39m \u001B[43m        \u001B[49m\u001B[43mreturn_offsets_mapping\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_offsets_mapping\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    629\u001B[39m \u001B[43m        \u001B[49m\u001B[43mreturn_length\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    630\u001B[39m \u001B[43m        \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m=\u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    631\u001B[39m \u001B[43m        \u001B[49m\u001B[43msplit_special_tokens\u001B[49m\u001B[43m=\u001B[49m\u001B[43msplit_special_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    632\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    633\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    635\u001B[39m     \u001B[38;5;66;03m# Return tensor is None, then we can remove the leading batch axis\u001B[39;00m\n\u001B[32m    636\u001B[39m     \u001B[38;5;66;03m# Overflowing tokens are returned as a batch of output so we keep them in this case\u001B[39;00m\n\u001B[32m    637\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m return_tensors \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m return_overflowing_tokens:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniforge3/envs/docuverse12/lib/python3.12/site-packages/transformers/models/roberta/tokenization_roberta_fast.py:217\u001B[39m, in \u001B[36mRobertaTokenizerFast._batch_encode_plus\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m    211\u001B[39m is_split_into_words = kwargs.get(\u001B[33m\"\u001B[39m\u001B[33mis_split_into_words\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[32m    212\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m.add_prefix_space \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_split_into_words, (\n\u001B[32m    213\u001B[39m     \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mYou need to instantiate \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.\u001B[34m__class__\u001B[39m.\u001B[34m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m with add_prefix_space=True \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    214\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mto use it with pretokenized inputs.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    215\u001B[39m )\n\u001B[32m--> \u001B[39m\u001B[32m217\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_batch_encode_plus\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniforge3/envs/docuverse12/lib/python3.12/site-packages/transformers/tokenization_utils_fast.py:527\u001B[39m, in \u001B[36mPreTrainedTokenizerFast._batch_encode_plus\u001B[39m\u001B[34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens)\u001B[39m\n\u001B[32m    522\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[32m    523\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mbatch_text_or_text_pairs has to be a list or a tuple (got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(batch_text_or_text_pairs)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m)\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    524\u001B[39m     )\n\u001B[32m    526\u001B[39m \u001B[38;5;66;03m# Set the truncation and padding strategy and restore the initial configuration\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m527\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mset_truncation_and_padding\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    528\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpadding_strategy\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpadding_strategy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    529\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtruncation_strategy\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtruncation_strategy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    530\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmax_length\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmax_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    531\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstride\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    532\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpad_to_multiple_of\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpad_to_multiple_of\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    533\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpadding_side\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpadding_side\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    534\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    536\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._tokenizer.encode_special_tokens != split_special_tokens:\n\u001B[32m    537\u001B[39m     \u001B[38;5;28mself\u001B[39m._tokenizer.encode_special_tokens = split_special_tokens\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniforge3/envs/docuverse12/lib/python3.12/site-packages/transformers/tokenization_utils_fast.py:480\u001B[39m, in \u001B[36mPreTrainedTokenizerFast.set_truncation_and_padding\u001B[39m\u001B[34m(self, padding_strategy, truncation_strategy, max_length, stride, pad_to_multiple_of, padding_side)\u001B[39m\n\u001B[32m    477\u001B[39m         current = {k: _truncation.get(k, \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m target}\n\u001B[32m    479\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m current != target:\n\u001B[32m--> \u001B[39m\u001B[32m480\u001B[39m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_tokenizer\u001B[49m\u001B[43m.\u001B[49m\u001B[43menable_truncation\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    482\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m padding_strategy == PaddingStrategy.DO_NOT_PAD:\n\u001B[32m    483\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m _padding \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[31mOverflowError\u001B[39m: can't convert negative int to unsigned"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "621e44dd69cbb1b2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
