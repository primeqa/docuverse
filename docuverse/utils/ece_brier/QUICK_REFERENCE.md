# Quick Reference Card

## üéØ One-Liner Commands

### Calculate Brier Score
```python
from brier_score import brier_score
score = brier_score(predictions, actuals)  # Lower is better (0-1)
```

### Calculate ECE
```python
from ece import expected_calibration_error
ece = expected_calibration_error(predictions, actuals, n_bins=10)  # Lower is better
```

### Generate Reliability Diagram
```python
from ece import plot_reliability_diagram
plot_reliability_diagram(predictions, actuals, save_path='plot.png')
```

### Complete Evaluation
```python
from example_evaluation import evaluate_reranker
metrics = evaluate_reranker(predictions, actuals, "MyModel")
```

## üìä Interpretation Thresholds

| Metric | Excellent | Good | Moderate | Poor |
|--------|-----------|------|----------|------|
| Brier Score | < 0.10 | 0.10-0.20 | 0.20-0.30 | > 0.30 |
| ECE | < 0.05 | 0.05-0.10 | 0.10-0.15 | > 0.15 |
| MCE | < 0.10 | 0.10-0.20 | 0.20-0.30 | > 0.30 |
| BSS | > 0.7 | 0.5-0.7 | 0.3-0.5 | < 0.3 |

## üîß Common Fixes

### High ECE (> 0.15)
**Problem:** Poor calibration
**Solution:** Temperature scaling
```python
calibrated_probs = sigmoid(logits / temperature)  # temperature > 1
```

### High Brier, Low ECE
**Problem:** Poor discrimination but well-calibrated
**Solution:** Improve model features/architecture

### Low Brier, High ECE
**Problem:** Good discrimination but poorly calibrated
**Solution:** Apply temperature scaling or Platt scaling

## üìÅ Files You Need

| Task | File |
|------|------|
| Start learning | [INDEX.md](INDEX.md) |
| Brier Score info | [README.md](README.md) |
| ECE info | [ECE_README.md](ECE_README.md) |
| Choose metrics | [METRICS_COMPARISON.md](METRICS_COMPARISON.md) |
| Code - Brier | [brier_score.py](brier_score.py) |
| Code - ECE | [ece.py](ece.py) |
| Complete example | [example_evaluation.py](example_evaluation.py) |

## ‚ö° Copy-Paste Snippets

### Minimal Evaluation
```python
from brier_score import brier_score
from ece import expected_calibration_error

bs = brier_score(preds, labels)
ece = expected_calibration_error(preds, labels)

if ece < 0.10 and bs < 0.15:
    print("‚úÖ Ready for production")
elif ece > 0.10:
    print("‚ö†Ô∏è Needs recalibration")
else:
    print("‚ö†Ô∏è Needs improvement")
```

### Production Monitoring
```python
from ece import expected_calibration_error
from brier_score import brier_score

ece = expected_calibration_error(recent_preds, recent_labels)
bs = brier_score(recent_preds, recent_labels)

if ece > 0.12 or bs > 0.25:
    send_alert("Model degraded")
    
log_metrics({'ece': ece, 'brier': bs})
```

### Model Comparison
```python
from example_evaluation import compare_models

models = {
    "BERT": bert_preds,
    "ColBERT": colbert_preds
}

compare_models(models, ground_truth)
```

## üéì Formula Cheat Sheet

### Brier Score
```
BS = (1/N) √ó Œ£(predicted - actual)¬≤
```
- Measures: Overall probability quality
- Range: [0, 1]
- Lower is better

### Expected Calibration Error
```
ECE = Œ£ (n_k/N) √ó |accuracy_k - confidence_k|
```
- Measures: Average calibration gap
- Range: [0, 1]
- Lower is better

### Maximum Calibration Error
```
MCE = max|accuracy_k - confidence_k|
```
- Measures: Worst-case calibration
- Range: [0, 1]
- Lower is better

### Brier Decomposition
```
Brier = Reliability - Resolution + Uncertainty
```
- Reliability: Calibration error (lower better)
- Resolution: Discrimination ability (higher better)
- Uncertainty: Inherent data variance

## üö® Common Mistakes

‚ùå **Using only Brier Score**
‚Üí Also check ECE for calibration-specific issues

‚ùå **Ignoring reliability diagrams**
‚Üí Visual inspection reveals patterns metrics miss

‚ùå **Not checking MCE**
‚Üí High MCE with low ECE means some bins are poorly calibrated

‚ùå **Wrong binning strategy**
‚Üí Use quantile binning for skewed predictions

‚ùå **Not recalibrating**
‚Üí If ECE > 0.10, apply temperature scaling

## üí° Pro Tips

1. ‚úÖ Report both Brier and ECE
2. ‚úÖ Include reliability diagram
3. ‚úÖ Use n_bins=10 (standard)
4. ‚úÖ Check decomposition for diagnosis
5. ‚úÖ Compare to baseline (BSS)
6. ‚úÖ Monitor in production
7. ‚úÖ Recalibrate if ECE > 0.10

## üìû Emergency Troubleshooting

| Symptom | Diagnosis | Fix |
|---------|-----------|-----|
| ECE > 0.15 | Miscalibrated | Temperature scaling |
| BS > 0.30 | Poor accuracy | Retrain model |
| MCE >> ECE | Some bins bad | Isotonic regression |
| Empty bins | Skewed preds | Use quantile binning |
| BSS < 0 | Worse than baseline | Check data quality |

## üéØ Decision Tree

```
Need to evaluate reranker?
‚îú‚îÄ Quick check? ‚Üí Use Brier + ECE
‚îú‚îÄ Full assessment? ‚Üí Use evaluate_reranker()
‚îú‚îÄ Compare models? ‚Üí Use compare_models()
‚îú‚îÄ Fix calibration? ‚Üí Temperature scaling
‚îî‚îÄ Monitor production? ‚Üí Track ECE over time
```

## üì¶ Installation

```bash
pip install numpy matplotlib
```

## üöÄ Getting Started (30 seconds)

```python
# 1. Import
from brier_score import brier_score
from ece import expected_calibration_error, plot_reliability_diagram

# 2. Evaluate
bs = brier_score(predictions, actuals)
ece = expected_calibration_error(predictions, actuals)

# 3. Visualize
plot_reliability_diagram(predictions, actuals, save_path='plot.png')

# 4. Decide
if ece < 0.10:
    print("‚úÖ Well-calibrated!")
else:
    print("‚ö†Ô∏è Apply temperature scaling")
```

---

**Print this page for quick reference! üìÑ**